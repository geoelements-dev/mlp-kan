{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1395ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "import time \n",
    "from termcolor import colored\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from networks import *\n",
    "import efficient_kan\n",
    "import kan\n",
    "from ChebyKAN import GeneralChebyKAN\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 9})\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "921aabe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = False\n",
    "save = True\n",
    "\n",
    "# model_parser = argparse.ArgumentParser(description='Specify model type.')\n",
    "# model_parser.add_argument('-model', dest='modeltype', type=str, default='original_kan',\n",
    "#                            help='Model type.', choices=['densenet', 'efficient_kan', 'original_kan'])\n",
    "# modeltype = model_parser.parse_args().modeltype\n",
    "# print(f\"Running with modeltype {modeltype}.\")\n",
    "\n",
    "modeltype = 'cheby_kan' #'original_kan' #\"efficient_kan\" # \"densenet\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a15f9ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = 0\n"
     ]
    }
   ],
   "source": [
    "if cluster == True:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-seed', dest='seed', type=int, default=0, help='Seed number.')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Print all the arguments\n",
    "    for arg in vars(args):\n",
    "        print(f'{arg}: {getattr(args, arg)}')\n",
    "\n",
    "    seed = args.seed\n",
    "    \n",
    "if cluster == False:\n",
    "    seed = 0 # Seed number.\n",
    "\n",
    "if save == True:\n",
    "    resultdir = os.path.join(os.getcwd(), 'DeepONet_results', 'seed='+str(seed)) \n",
    "    if not os.path.exists(resultdir):\n",
    "        os.makedirs(resultdir)\n",
    "\n",
    "if save == True and cluster == True:\n",
    "    orig_stdout = sys.stdout\n",
    "    q = open(os.path.join(resultdir, 'output-'+'seed='+str(seed)+'.txt'), 'w')\n",
    "    sys.stdout = q\n",
    "    print (\"------START------\")\n",
    "\n",
    "print('seed = '+str(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f7126e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3d11f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea37c44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 101)\n",
      "(2500, 101)\n",
      "(2500, 101, 101)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data = loadmat('data/Burger.mat') # Load the .mat file\n",
    "#print(data)\n",
    "print(data['tspan'].shape)\n",
    "print(data['input'].shape)  # Initial conditions: Gaussian random fields, Nsamples x 101, each IC sample is (1 x 101)\n",
    "print(data['output'].shape) # Time evolution of the solution field: Nsamples x 101 x 101.\n",
    "                             # Each field is 101 x 101, rows correspond to time and columns respond to location.\n",
    "                             # First row corresponds to solution at t=0 (1st time step)\n",
    "                             # and next  row corresponds to solution at t=0.01 (2nd time step) and so on.\n",
    "                             # last row correspond to solution at t=1 (101th time step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6609428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nt = 101 , nx = 101\n",
      "Shape of t-span and x-span: torch.Size([101]) torch.Size([101])\n",
      "t-span: tensor([0.0000, 0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700, 0.0800,\n",
      "        0.0900, 0.1000, 0.1100, 0.1200, 0.1300, 0.1400, 0.1500, 0.1600, 0.1700,\n",
      "        0.1800, 0.1900, 0.2000, 0.2100, 0.2200, 0.2300, 0.2400, 0.2500, 0.2600,\n",
      "        0.2700, 0.2800, 0.2900, 0.3000, 0.3100, 0.3200, 0.3300, 0.3400, 0.3500,\n",
      "        0.3600, 0.3700, 0.3800, 0.3900, 0.4000, 0.4100, 0.4200, 0.4300, 0.4400,\n",
      "        0.4500, 0.4600, 0.4700, 0.4800, 0.4900, 0.5000, 0.5100, 0.5200, 0.5300,\n",
      "        0.5400, 0.5500, 0.5600, 0.5700, 0.5800, 0.5900, 0.6000, 0.6100, 0.6200,\n",
      "        0.6300, 0.6400, 0.6500, 0.6600, 0.6700, 0.6800, 0.6900, 0.7000, 0.7100,\n",
      "        0.7200, 0.7300, 0.7400, 0.7500, 0.7600, 0.7700, 0.7800, 0.7900, 0.8000,\n",
      "        0.8100, 0.8200, 0.8300, 0.8400, 0.8500, 0.8600, 0.8700, 0.8800, 0.8900,\n",
      "        0.9000, 0.9100, 0.9200, 0.9300, 0.9400, 0.9500, 0.9600, 0.9700, 0.9800,\n",
      "        0.9900, 1.0000])\n",
      "x-span: tensor([0.0000, 0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700, 0.0800,\n",
      "        0.0900, 0.1000, 0.1100, 0.1200, 0.1300, 0.1400, 0.1500, 0.1600, 0.1700,\n",
      "        0.1800, 0.1900, 0.2000, 0.2100, 0.2200, 0.2300, 0.2400, 0.2500, 0.2600,\n",
      "        0.2700, 0.2800, 0.2900, 0.3000, 0.3100, 0.3200, 0.3300, 0.3400, 0.3500,\n",
      "        0.3600, 0.3700, 0.3800, 0.3900, 0.4000, 0.4100, 0.4200, 0.4300, 0.4400,\n",
      "        0.4500, 0.4600, 0.4700, 0.4800, 0.4900, 0.5000, 0.5100, 0.5200, 0.5300,\n",
      "        0.5400, 0.5500, 0.5600, 0.5700, 0.5800, 0.5900, 0.6000, 0.6100, 0.6200,\n",
      "        0.6300, 0.6400, 0.6500, 0.6600, 0.6700, 0.6800, 0.6900, 0.7000, 0.7100,\n",
      "        0.7200, 0.7300, 0.7400, 0.7500, 0.7600, 0.7700, 0.7800, 0.7900, 0.8000,\n",
      "        0.8100, 0.8200, 0.8300, 0.8400, 0.8500, 0.8600, 0.8700, 0.8800, 0.8900,\n",
      "        0.9000, 0.9100, 0.9200, 0.9300, 0.9400, 0.9500, 0.9600, 0.9700, 0.9800,\n",
      "        0.9900, 1.0000])\n",
      "Shape of grid: torch.Size([10201, 2])\n",
      "grid: tensor([[0.0000, 0.0000],\n",
      "        [0.0000, 0.0100],\n",
      "        [0.0000, 0.0200],\n",
      "        ...,\n",
      "        [1.0000, 0.9800],\n",
      "        [1.0000, 0.9900],\n",
      "        [1.0000, 1.0000]])\n",
      "Shape of inputs_train: torch.Size([2000, 101])\n",
      "Shape of inputs_test: torch.Size([500, 101])\n",
      "Shape of outputs_train: torch.Size([2000, 101, 101])\n",
      "Shape of outputs_test: torch.Size([500, 101, 101])\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "# Convert NumPy arrays to PyTorch tensors\n",
    "inputs = torch.from_numpy(data['input']).float().to(device)\n",
    "outputs = torch.from_numpy(data['output']).float().to(device)\n",
    "\n",
    "t_span = torch.from_numpy(data['tspan'].flatten()).float().to(device)\n",
    "x_span = torch.linspace(0, 1, data['output'].shape[2]).float().to(device)\n",
    "nt, nx = len(t_span), len(x_span) # number of discretizations in time and location.\n",
    "print(\"nt =\",nt, \", nx =\",nx)\n",
    "print(\"Shape of t-span and x-span:\",t_span.shape, x_span.shape)\n",
    "print(\"t-span:\", t_span)\n",
    "print(\"x-span:\", x_span)\n",
    "\n",
    "# Estimating grid points\n",
    "T, X = torch.meshgrid(t_span, x_span)\n",
    "# print(T)\n",
    "# print(X)\n",
    "grid = torch.vstack((T.flatten(), X.flatten())).T\n",
    "print(\"Shape of grid:\", grid.shape) # (nt*nx, 2)\n",
    "print(\"grid:\", grid) # time, location\n",
    "\n",
    "# Split the data into training (2000) and testing (500) samples\n",
    "inputs_train, inputs_test, outputs_train, outputs_test = train_test_split(inputs, outputs, test_size=500, random_state=seed)\n",
    "\n",
    "# Check the shapes of the subsets\n",
    "print(\"Shape of inputs_train:\", inputs_train.shape)\n",
    "print(\"Shape of inputs_test:\", inputs_test.shape)\n",
    "print(\"Shape of outputs_train:\", outputs_train.shape)\n",
    "print(\"Shape of outputs_test:\", outputs_test.shape)\n",
    "print('#'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cc38415",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepONet(nn.Module):\n",
    "\n",
    "    def __init__(self, branch_net, trunk_net):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.branch_net = branch_net\n",
    "        self.trunk_net = trunk_net\n",
    "    \n",
    "    def forward(self, branch_inputs, trunk_inputs):\n",
    "        \"\"\"\n",
    "        bs    :  Batch size.\n",
    "        m     :  Number of sensors on each input IC field. # IC:initial condition\n",
    "        neval :  Number of points at which output field is evaluated for a given input IC field sample = nt*nx\n",
    "        p     :  Number of output neurons in both branch and trunk net.   \n",
    "        \n",
    "        branch inputs shape: (bs, m) \n",
    "        trunk inputs shape : (neval, 2) # 2 corresponds to t and x\n",
    "        \n",
    "        shapes:  inputs shape         -->      outputs shape\n",
    "        branch:  (bs x m)             -->      (bs x p)\n",
    "        trunk:   (neval x 2)          -->      (neval x p)\n",
    "        \n",
    "        outputs shape: (bs x neval).\n",
    "        \"\"\"\n",
    "        \n",
    "        branch_outputs = self.branch_net(branch_inputs)\n",
    "        trunk_outputs = self.trunk_net(trunk_inputs)\n",
    "        \n",
    "        results = torch.einsum('ik, lk -> il', branch_outputs, trunk_outputs)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d28bdffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sin(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sin, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1ae139a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRANCH-NET SUMMARY:\n",
      "####################################################################################################\n",
      "TRUNK-NET SUMMARY:\n",
      "####################################################################################################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeepONet(\n",
       "  (branch_net): KAN(\n",
       "    (layers): ModuleList(\n",
       "      (0): KANLinear(\n",
       "        (base_activation): SiLU()\n",
       "      )\n",
       "      (1): KANLinear(\n",
       "        (base_activation): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (trunk_net): KAN(\n",
       "    (layers): ModuleList(\n",
       "      (0): KANLinear(\n",
       "        (base_activation): SiLU()\n",
       "      )\n",
       "      (1): KANLinear(\n",
       "        (base_activation): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "input_neurons_branch: Number of input neurons in the branch net.\n",
    "input_neurons_trunk: Number of input neurons in the trunk net.\n",
    "p: Number of output neurons in both the branch and trunk net.\n",
    "\"\"\"\n",
    "p = 100 # Number of output neurons in both the branch and trunk net.\n",
    "\n",
    "input_neurons_branch = nx # m\n",
    "if modeltype == 'efficient_kan':\n",
    "    # branch_net = efficient_kan.KAN(layers_hidden=[input_neurons_branch] + [100]*6 + [p])\n",
    "    branch_net = efficient_kan.KAN(layers_hidden=[input_neurons_branch] + [2*input_neurons_branch+1]*1 + [p])\n",
    "elif modeltype == 'original_kan':\n",
    "    branch_net = kan.KAN(width=[input_neurons_branch,2*input_neurons_branch+1,p], grid=5, k=3, seed=0)\n",
    "elif modeltype == 'cheby_kan':\n",
    "    branch_net = GeneralChebyKAN(layer_dims=[input_neurons_branch,2*input_neurons_branch+1,p], degree=4)\n",
    "else:\n",
    "    branch_net = DenseNet(layersizes=[input_neurons_branch] + [100]*6 + [p], activation=nn.SiLU()) #nn.LeakyReLU() #nn.Tanh()\n",
    "branch_net.to(device)\n",
    "# print(branch_net)\n",
    "print('BRANCH-NET SUMMARY:')\n",
    "# summary(branch_net, input_size=(input_neurons_branch,))  \n",
    "print('#'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18595b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2 corresponds to t and x\n",
    "input_neurons_trunk = 2\n",
    "if modeltype == 'efficient_kan':\n",
    "    # trunk_net = efficient_kan.KAN(layers_hidden=[input_neurons_trunk] + [100]*6 + [p]) \n",
    "    trunk_net = efficient_kan.KAN(layers_hidden=[input_neurons_trunk] + [input_neurons_trunk*2+1]*1 + [p]) \n",
    "elif modeltype == 'original_kan':\n",
    "    trunk_net = kan.KAN(width=[input_neurons_trunk,2*input_neurons_trunk+1,p], grid=5, k=3, seed=0)\n",
    "elif modeltype == 'cheby_kan':\n",
    "    trunk_net = GeneralChebyKAN(layer_dims=[input_neurons_trunk,2*input_neurons_trunk+1,p], degree=4)\n",
    "else:\n",
    "    trunk_net = DenseNet(layersizes=[input_neurons_trunk] + [100]*6 + [p], activation=nn.SiLU()) #nn.LeakyReLU() #nn.Tanh()\n",
    "trunk_net.to(device)\n",
    "# print(trunk_net)\n",
    "print('TRUNK-NET SUMMARY:')\n",
    "# summary(trunk_net, input_size=(input_neurons_trunk,))\n",
    "print('#'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99db4630",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepONet(branch_net, trunk_net)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "548e3c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of learnable parameters: 413130\n"
     ]
    }
   ],
   "source": [
    "def count_learnable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "num_learnable_parameters = count_learnable_parameters(branch_net) + count_learnable_parameters(trunk_net)\n",
    "print(\"Total number of learnable parameters:\", num_learnable_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bc7bb3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data\n",
      "torch.Size([2000, 101]) torch.Size([2000, 101, 101])\n",
      "####################################################################################################\n",
      "Epoch 0: Batch 0: loss = 0.026900, learning rate = 0.001000\n",
      "Epoch 0: Batch 1: loss = 0.029907, learning rate = 0.001000\n",
      "Epoch 0: Batch 2: loss = 0.028174, learning rate = 0.001000\n",
      "Epoch 0: Batch 3: loss = 0.026236, learning rate = 0.001000\n",
      "Epoch 0: Batch 4: loss = 0.028345, learning rate = 0.001000\n",
      "Epoch 0: Batch 5: loss = 0.026452, learning rate = 0.001000\n",
      "Epoch 0: Batch 6: loss = 0.025115, learning rate = 0.001000\n",
      "Epoch 0: Batch 7: loss = 0.025534, learning rate = 0.001000\n",
      "Epoch 0: Batch 8: loss = 0.021131, learning rate = 0.001000\n",
      "Epoch 0: Batch 9: loss = 0.025244, learning rate = 0.001000\n",
      "Epoch 0: Batch 10: loss = 0.026844, learning rate = 0.001000\n",
      "Epoch 0: Batch 11: loss = 0.020460, learning rate = 0.001000\n",
      "Epoch 0: Batch 12: loss = 0.021535, learning rate = 0.001000\n",
      "Epoch 0: Batch 13: loss = 0.020673, learning rate = 0.001000\n",
      "Epoch 0: Batch 14: loss = 0.025753, learning rate = 0.001000\n",
      "Epoch 0: Batch 15: loss = 0.017106, learning rate = 0.001000\n",
      "Epoch 0: Batch 16: loss = 0.018576, learning rate = 0.001000\n",
      "Epoch 0: Batch 17: loss = 0.023615, learning rate = 0.001000\n",
      "Epoch 0: Batch 18: loss = 0.026520, learning rate = 0.001000\n",
      "Epoch 0: Batch 19: loss = 0.020443, learning rate = 0.001000\n",
      "Epoch 0: Batch 20: loss = 0.022044, learning rate = 0.001000\n",
      "Epoch 0: Batch 21: loss = 0.018989, learning rate = 0.001000\n",
      "Epoch 0: Batch 22: loss = 0.018936, learning rate = 0.001000\n",
      "Epoch 0: Batch 23: loss = 0.024648, learning rate = 0.001000\n",
      "Epoch 0: Batch 24: loss = 0.016812, learning rate = 0.001000\n",
      "Epoch 0: Batch 25: loss = 0.021856, learning rate = 0.001000\n",
      "Epoch 0: Batch 26: loss = 0.021313, learning rate = 0.001000\n",
      "Epoch 0: Batch 27: loss = 0.023313, learning rate = 0.001000\n",
      "Epoch 0: Batch 28: loss = 0.022657, learning rate = 0.001000\n",
      "Epoch 0: Batch 29: loss = 0.020378, learning rate = 0.001000\n",
      "Epoch 0: Batch 30: loss = 0.018216, learning rate = 0.001000\n",
      "Epoch 0: Batch 31: loss = 0.017359, learning rate = 0.001000\n",
      "Epoch 50: Batch 0: loss = 0.001153, learning rate = 0.001000\n",
      "Epoch 50: Batch 1: loss = 0.000888, learning rate = 0.001000\n",
      "Epoch 50: Batch 2: loss = 0.001005, learning rate = 0.001000\n",
      "Epoch 50: Batch 3: loss = 0.000846, learning rate = 0.001000\n",
      "Epoch 50: Batch 4: loss = 0.001107, learning rate = 0.001000\n",
      "Epoch 50: Batch 5: loss = 0.000764, learning rate = 0.001000\n",
      "Epoch 50: Batch 6: loss = 0.000757, learning rate = 0.001000\n",
      "Epoch 50: Batch 7: loss = 0.001314, learning rate = 0.001000\n",
      "Epoch 50: Batch 8: loss = 0.000940, learning rate = 0.001000\n",
      "Epoch 50: Batch 9: loss = 0.000767, learning rate = 0.001000\n",
      "Epoch 50: Batch 10: loss = 0.000887, learning rate = 0.001000\n",
      "Epoch 50: Batch 11: loss = 0.000884, learning rate = 0.001000\n",
      "Epoch 50: Batch 12: loss = 0.000752, learning rate = 0.001000\n",
      "Epoch 50: Batch 13: loss = 0.001168, learning rate = 0.001000\n",
      "Epoch 50: Batch 14: loss = 0.000891, learning rate = 0.001000\n",
      "Epoch 50: Batch 15: loss = 0.000818, learning rate = 0.001000\n",
      "Epoch 50: Batch 16: loss = 0.001222, learning rate = 0.001000\n",
      "Epoch 50: Batch 17: loss = 0.000857, learning rate = 0.001000\n",
      "Epoch 50: Batch 18: loss = 0.000897, learning rate = 0.001000\n",
      "Epoch 50: Batch 19: loss = 0.000930, learning rate = 0.001000\n",
      "Epoch 50: Batch 20: loss = 0.000691, learning rate = 0.001000\n",
      "Epoch 50: Batch 21: loss = 0.001068, learning rate = 0.001000\n",
      "Epoch 50: Batch 22: loss = 0.000717, learning rate = 0.001000\n",
      "Epoch 50: Batch 23: loss = 0.001031, learning rate = 0.001000\n",
      "Epoch 50: Batch 24: loss = 0.000995, learning rate = 0.001000\n",
      "Epoch 50: Batch 25: loss = 0.000752, learning rate = 0.001000\n",
      "Epoch 50: Batch 26: loss = 0.000800, learning rate = 0.001000\n",
      "Epoch 50: Batch 27: loss = 0.000691, learning rate = 0.001000\n",
      "Epoch 50: Batch 28: loss = 0.001173, learning rate = 0.001000\n",
      "Epoch 50: Batch 29: loss = 0.000699, learning rate = 0.001000\n",
      "Epoch 50: Batch 30: loss = 0.001107, learning rate = 0.001000\n",
      "Epoch 50: Batch 31: loss = 0.002047, learning rate = 0.001000\n",
      "Epoch 100: Batch 0: loss = 0.000653, learning rate = 0.001000\n",
      "Epoch 100: Batch 1: loss = 0.000518, learning rate = 0.001000\n",
      "Epoch 100: Batch 2: loss = 0.000490, learning rate = 0.001000\n",
      "Epoch 100: Batch 3: loss = 0.000723, learning rate = 0.001000\n",
      "Epoch 100: Batch 4: loss = 0.000587, learning rate = 0.001000\n",
      "Epoch 100: Batch 5: loss = 0.000710, learning rate = 0.001000\n",
      "Epoch 100: Batch 6: loss = 0.000568, learning rate = 0.001000\n",
      "Epoch 100: Batch 7: loss = 0.000632, learning rate = 0.001000\n",
      "Epoch 100: Batch 8: loss = 0.000679, learning rate = 0.001000\n",
      "Epoch 100: Batch 9: loss = 0.000876, learning rate = 0.001000\n",
      "Epoch 100: Batch 10: loss = 0.000684, learning rate = 0.001000\n",
      "Epoch 100: Batch 11: loss = 0.000505, learning rate = 0.001000\n",
      "Epoch 100: Batch 12: loss = 0.000757, learning rate = 0.001000\n",
      "Epoch 100: Batch 13: loss = 0.000708, learning rate = 0.001000\n",
      "Epoch 100: Batch 14: loss = 0.000580, learning rate = 0.001000\n",
      "Epoch 100: Batch 15: loss = 0.000929, learning rate = 0.001000\n",
      "Epoch 100: Batch 16: loss = 0.000652, learning rate = 0.001000\n",
      "Epoch 100: Batch 17: loss = 0.000626, learning rate = 0.001000\n",
      "Epoch 100: Batch 18: loss = 0.000662, learning rate = 0.001000\n",
      "Epoch 100: Batch 19: loss = 0.000681, learning rate = 0.001000\n",
      "Epoch 100: Batch 20: loss = 0.000600, learning rate = 0.001000\n",
      "Epoch 100: Batch 21: loss = 0.000692, learning rate = 0.001000\n",
      "Epoch 100: Batch 22: loss = 0.000469, learning rate = 0.001000\n",
      "Epoch 100: Batch 23: loss = 0.000481, learning rate = 0.001000\n",
      "Epoch 100: Batch 24: loss = 0.000666, learning rate = 0.001000\n",
      "Epoch 100: Batch 25: loss = 0.000895, learning rate = 0.001000\n",
      "Epoch 100: Batch 26: loss = 0.000848, learning rate = 0.001000\n",
      "Epoch 100: Batch 27: loss = 0.000914, learning rate = 0.001000\n",
      "Epoch 100: Batch 28: loss = 0.000621, learning rate = 0.001000\n",
      "Epoch 100: Batch 29: loss = 0.000605, learning rate = 0.001000\n",
      "Epoch 100: Batch 30: loss = 0.000754, learning rate = 0.001000\n",
      "Epoch 100: Batch 31: loss = 0.000882, learning rate = 0.001000\n",
      "Epoch 150: Batch 0: loss = 0.000999, learning rate = 0.001000\n",
      "Epoch 150: Batch 1: loss = 0.000724, learning rate = 0.001000\n",
      "Epoch 150: Batch 2: loss = 0.000588, learning rate = 0.001000\n",
      "Epoch 150: Batch 3: loss = 0.000741, learning rate = 0.001000\n",
      "Epoch 150: Batch 4: loss = 0.000724, learning rate = 0.001000\n",
      "Epoch 150: Batch 5: loss = 0.000714, learning rate = 0.001000\n",
      "Epoch 150: Batch 6: loss = 0.000937, learning rate = 0.001000\n",
      "Epoch 150: Batch 7: loss = 0.000748, learning rate = 0.001000\n",
      "Epoch 150: Batch 8: loss = 0.000939, learning rate = 0.001000\n",
      "Epoch 150: Batch 9: loss = 0.000662, learning rate = 0.001000\n",
      "Epoch 150: Batch 10: loss = 0.000661, learning rate = 0.001000\n",
      "Epoch 150: Batch 11: loss = 0.000759, learning rate = 0.001000\n",
      "Epoch 150: Batch 12: loss = 0.000686, learning rate = 0.001000\n",
      "Epoch 150: Batch 13: loss = 0.000977, learning rate = 0.001000\n",
      "Epoch 150: Batch 14: loss = 0.000845, learning rate = 0.001000\n",
      "Epoch 150: Batch 15: loss = 0.000793, learning rate = 0.001000\n",
      "Epoch 150: Batch 16: loss = 0.000636, learning rate = 0.001000\n",
      "Epoch 150: Batch 17: loss = 0.000674, learning rate = 0.001000\n",
      "Epoch 150: Batch 18: loss = 0.000596, learning rate = 0.001000\n",
      "Epoch 150: Batch 19: loss = 0.000636, learning rate = 0.001000\n",
      "Epoch 150: Batch 20: loss = 0.000609, learning rate = 0.001000\n",
      "Epoch 150: Batch 21: loss = 0.000764, learning rate = 0.001000\n",
      "Epoch 150: Batch 22: loss = 0.000902, learning rate = 0.001000\n",
      "Epoch 150: Batch 23: loss = 0.000791, learning rate = 0.001000\n",
      "Epoch 150: Batch 24: loss = 0.000693, learning rate = 0.001000\n",
      "Epoch 150: Batch 25: loss = 0.000635, learning rate = 0.001000\n",
      "Epoch 150: Batch 26: loss = 0.000649, learning rate = 0.001000\n",
      "Epoch 150: Batch 27: loss = 0.000556, learning rate = 0.001000\n",
      "Epoch 150: Batch 28: loss = 0.000516, learning rate = 0.001000\n",
      "Epoch 150: Batch 29: loss = 0.000678, learning rate = 0.001000\n",
      "Epoch 150: Batch 30: loss = 0.000460, learning rate = 0.001000\n",
      "Epoch 150: Batch 31: loss = 0.000534, learning rate = 0.001000\n",
      "Epoch 200: Batch 0: loss = 0.000721, learning rate = 0.001000\n",
      "Epoch 200: Batch 1: loss = 0.000980, learning rate = 0.001000\n",
      "Epoch 200: Batch 2: loss = 0.000672, learning rate = 0.001000\n",
      "Epoch 200: Batch 3: loss = 0.000742, learning rate = 0.001000\n",
      "Epoch 200: Batch 4: loss = 0.000892, learning rate = 0.001000\n",
      "Epoch 200: Batch 5: loss = 0.001032, learning rate = 0.001000\n",
      "Epoch 200: Batch 6: loss = 0.000660, learning rate = 0.001000\n",
      "Epoch 200: Batch 7: loss = 0.000853, learning rate = 0.001000\n",
      "Epoch 200: Batch 8: loss = 0.000620, learning rate = 0.001000\n",
      "Epoch 200: Batch 9: loss = 0.000694, learning rate = 0.001000\n",
      "Epoch 200: Batch 10: loss = 0.000924, learning rate = 0.001000\n",
      "Epoch 200: Batch 11: loss = 0.000488, learning rate = 0.001000\n",
      "Epoch 200: Batch 12: loss = 0.000994, learning rate = 0.001000\n",
      "Epoch 200: Batch 13: loss = 0.000658, learning rate = 0.001000\n",
      "Epoch 200: Batch 14: loss = 0.000471, learning rate = 0.001000\n",
      "Epoch 200: Batch 15: loss = 0.000927, learning rate = 0.001000\n",
      "Epoch 200: Batch 16: loss = 0.000562, learning rate = 0.001000\n",
      "Epoch 200: Batch 17: loss = 0.000627, learning rate = 0.001000\n",
      "Epoch 200: Batch 18: loss = 0.000856, learning rate = 0.001000\n",
      "Epoch 200: Batch 19: loss = 0.000488, learning rate = 0.001000\n",
      "Epoch 200: Batch 20: loss = 0.000540, learning rate = 0.001000\n",
      "Epoch 200: Batch 21: loss = 0.001077, learning rate = 0.001000\n",
      "Epoch 200: Batch 22: loss = 0.000457, learning rate = 0.001000\n",
      "Epoch 200: Batch 23: loss = 0.000689, learning rate = 0.001000\n",
      "Epoch 200: Batch 24: loss = 0.000867, learning rate = 0.001000\n",
      "Epoch 200: Batch 25: loss = 0.000805, learning rate = 0.001000\n",
      "Epoch 200: Batch 26: loss = 0.000567, learning rate = 0.001000\n",
      "Epoch 200: Batch 27: loss = 0.001094, learning rate = 0.001000\n",
      "Epoch 200: Batch 28: loss = 0.000856, learning rate = 0.001000\n",
      "Epoch 200: Batch 29: loss = 0.000671, learning rate = 0.001000\n",
      "Epoch 200: Batch 30: loss = 0.000842, learning rate = 0.001000\n",
      "Epoch 200: Batch 31: loss = 0.001636, learning rate = 0.001000\n",
      "Epoch 250: Batch 0: loss = 0.000335, learning rate = 0.001000\n",
      "Epoch 250: Batch 1: loss = 0.000342, learning rate = 0.001000\n",
      "Epoch 250: Batch 2: loss = 0.000534, learning rate = 0.001000\n",
      "Epoch 250: Batch 3: loss = 0.000526, learning rate = 0.001000\n",
      "Epoch 250: Batch 4: loss = 0.000424, learning rate = 0.001000\n",
      "Epoch 250: Batch 5: loss = 0.000270, learning rate = 0.001000\n",
      "Epoch 250: Batch 6: loss = 0.000239, learning rate = 0.001000\n",
      "Epoch 250: Batch 7: loss = 0.000412, learning rate = 0.001000\n",
      "Epoch 250: Batch 8: loss = 0.000354, learning rate = 0.001000\n",
      "Epoch 250: Batch 9: loss = 0.000408, learning rate = 0.001000\n",
      "Epoch 250: Batch 10: loss = 0.000443, learning rate = 0.001000\n",
      "Epoch 250: Batch 11: loss = 0.000384, learning rate = 0.001000\n",
      "Epoch 250: Batch 12: loss = 0.000336, learning rate = 0.001000\n",
      "Epoch 250: Batch 13: loss = 0.000418, learning rate = 0.001000\n",
      "Epoch 250: Batch 14: loss = 0.000531, learning rate = 0.001000\n",
      "Epoch 250: Batch 15: loss = 0.000418, learning rate = 0.001000\n",
      "Epoch 250: Batch 16: loss = 0.000330, learning rate = 0.001000\n",
      "Epoch 250: Batch 17: loss = 0.000241, learning rate = 0.001000\n",
      "Epoch 250: Batch 18: loss = 0.000397, learning rate = 0.001000\n",
      "Epoch 250: Batch 19: loss = 0.000236, learning rate = 0.001000\n",
      "Epoch 250: Batch 20: loss = 0.000400, learning rate = 0.001000\n",
      "Epoch 250: Batch 21: loss = 0.000232, learning rate = 0.001000\n",
      "Epoch 250: Batch 22: loss = 0.000580, learning rate = 0.001000\n",
      "Epoch 250: Batch 23: loss = 0.000422, learning rate = 0.001000\n",
      "Epoch 250: Batch 24: loss = 0.000372, learning rate = 0.001000\n",
      "Epoch 250: Batch 25: loss = 0.000301, learning rate = 0.001000\n",
      "Epoch 250: Batch 26: loss = 0.000374, learning rate = 0.001000\n",
      "Epoch 250: Batch 27: loss = 0.000437, learning rate = 0.001000\n",
      "Epoch 250: Batch 28: loss = 0.000336, learning rate = 0.001000\n",
      "Epoch 250: Batch 29: loss = 0.000275, learning rate = 0.001000\n",
      "Epoch 250: Batch 30: loss = 0.000212, learning rate = 0.001000\n",
      "Epoch 250: Batch 31: loss = 0.000526, learning rate = 0.001000\n",
      "Epoch 300: Batch 0: loss = 0.000569, learning rate = 0.001000\n",
      "Epoch 300: Batch 1: loss = 0.000331, learning rate = 0.001000\n",
      "Epoch 300: Batch 2: loss = 0.000259, learning rate = 0.001000\n",
      "Epoch 300: Batch 3: loss = 0.000240, learning rate = 0.001000\n",
      "Epoch 300: Batch 4: loss = 0.000599, learning rate = 0.001000\n",
      "Epoch 300: Batch 5: loss = 0.000403, learning rate = 0.001000\n",
      "Epoch 300: Batch 6: loss = 0.000205, learning rate = 0.001000\n",
      "Epoch 300: Batch 7: loss = 0.000373, learning rate = 0.001000\n",
      "Epoch 300: Batch 8: loss = 0.000504, learning rate = 0.001000\n",
      "Epoch 300: Batch 9: loss = 0.000376, learning rate = 0.001000\n",
      "Epoch 300: Batch 10: loss = 0.000395, learning rate = 0.001000\n",
      "Epoch 300: Batch 11: loss = 0.000277, learning rate = 0.001000\n",
      "Epoch 300: Batch 12: loss = 0.000400, learning rate = 0.001000\n",
      "Epoch 300: Batch 13: loss = 0.000295, learning rate = 0.001000\n",
      "Epoch 300: Batch 14: loss = 0.000261, learning rate = 0.001000\n",
      "Epoch 300: Batch 15: loss = 0.000354, learning rate = 0.001000\n",
      "Epoch 300: Batch 16: loss = 0.000221, learning rate = 0.001000\n",
      "Epoch 300: Batch 17: loss = 0.000427, learning rate = 0.001000\n",
      "Epoch 300: Batch 18: loss = 0.000424, learning rate = 0.001000\n",
      "Epoch 300: Batch 19: loss = 0.000279, learning rate = 0.001000\n",
      "Epoch 300: Batch 20: loss = 0.000243, learning rate = 0.001000\n",
      "Epoch 300: Batch 21: loss = 0.000315, learning rate = 0.001000\n",
      "Epoch 300: Batch 22: loss = 0.000434, learning rate = 0.001000\n",
      "Epoch 300: Batch 23: loss = 0.000314, learning rate = 0.001000\n",
      "Epoch 300: Batch 24: loss = 0.000321, learning rate = 0.001000\n",
      "Epoch 300: Batch 25: loss = 0.000295, learning rate = 0.001000\n",
      "Epoch 300: Batch 26: loss = 0.000389, learning rate = 0.001000\n",
      "Epoch 300: Batch 27: loss = 0.000233, learning rate = 0.001000\n",
      "Epoch 300: Batch 28: loss = 0.000377, learning rate = 0.001000\n",
      "Epoch 300: Batch 29: loss = 0.000305, learning rate = 0.001000\n",
      "Epoch 300: Batch 30: loss = 0.000416, learning rate = 0.001000\n",
      "Epoch 300: Batch 31: loss = 0.000574, learning rate = 0.001000\n",
      "Epoch 350: Batch 0: loss = 0.000449, learning rate = 0.001000\n",
      "Epoch 350: Batch 1: loss = 0.000472, learning rate = 0.001000\n",
      "Epoch 350: Batch 2: loss = 0.000319, learning rate = 0.001000\n",
      "Epoch 350: Batch 3: loss = 0.000354, learning rate = 0.001000\n",
      "Epoch 350: Batch 4: loss = 0.000378, learning rate = 0.001000\n",
      "Epoch 350: Batch 5: loss = 0.000368, learning rate = 0.001000\n",
      "Epoch 350: Batch 6: loss = 0.000437, learning rate = 0.001000\n",
      "Epoch 350: Batch 7: loss = 0.000216, learning rate = 0.001000\n",
      "Epoch 350: Batch 8: loss = 0.000254, learning rate = 0.001000\n",
      "Epoch 350: Batch 9: loss = 0.000231, learning rate = 0.001000\n",
      "Epoch 350: Batch 10: loss = 0.000412, learning rate = 0.001000\n",
      "Epoch 350: Batch 11: loss = 0.000296, learning rate = 0.001000\n",
      "Epoch 350: Batch 12: loss = 0.000345, learning rate = 0.001000\n",
      "Epoch 350: Batch 13: loss = 0.000258, learning rate = 0.001000\n",
      "Epoch 350: Batch 14: loss = 0.000383, learning rate = 0.001000\n",
      "Epoch 350: Batch 15: loss = 0.000509, learning rate = 0.001000\n",
      "Epoch 350: Batch 16: loss = 0.000336, learning rate = 0.001000\n",
      "Epoch 350: Batch 17: loss = 0.000309, learning rate = 0.001000\n",
      "Epoch 350: Batch 18: loss = 0.000498, learning rate = 0.001000\n",
      "Epoch 350: Batch 19: loss = 0.000316, learning rate = 0.001000\n",
      "Epoch 350: Batch 20: loss = 0.000293, learning rate = 0.001000\n",
      "Epoch 350: Batch 21: loss = 0.000268, learning rate = 0.001000\n",
      "Epoch 350: Batch 22: loss = 0.000272, learning rate = 0.001000\n",
      "Epoch 350: Batch 23: loss = 0.000328, learning rate = 0.001000\n",
      "Epoch 350: Batch 24: loss = 0.000409, learning rate = 0.001000\n",
      "Epoch 350: Batch 25: loss = 0.000305, learning rate = 0.001000\n",
      "Epoch 350: Batch 26: loss = 0.000456, learning rate = 0.001000\n",
      "Epoch 350: Batch 27: loss = 0.000221, learning rate = 0.001000\n",
      "Epoch 350: Batch 28: loss = 0.000278, learning rate = 0.001000\n",
      "Epoch 350: Batch 29: loss = 0.000272, learning rate = 0.001000\n",
      "Epoch 350: Batch 30: loss = 0.000230, learning rate = 0.001000\n",
      "Epoch 350: Batch 31: loss = 0.000578, learning rate = 0.001000\n",
      "Epoch 400: Batch 0: loss = 0.000296, learning rate = 0.001000\n",
      "Epoch 400: Batch 1: loss = 0.000313, learning rate = 0.001000\n",
      "Epoch 400: Batch 2: loss = 0.000295, learning rate = 0.001000\n",
      "Epoch 400: Batch 3: loss = 0.000458, learning rate = 0.001000\n",
      "Epoch 400: Batch 4: loss = 0.000346, learning rate = 0.001000\n",
      "Epoch 400: Batch 5: loss = 0.000342, learning rate = 0.001000\n",
      "Epoch 400: Batch 6: loss = 0.000370, learning rate = 0.001000\n",
      "Epoch 400: Batch 7: loss = 0.000243, learning rate = 0.001000\n",
      "Epoch 400: Batch 8: loss = 0.000318, learning rate = 0.001000\n",
      "Epoch 400: Batch 9: loss = 0.000348, learning rate = 0.001000\n",
      "Epoch 400: Batch 10: loss = 0.000220, learning rate = 0.001000\n",
      "Epoch 400: Batch 11: loss = 0.000510, learning rate = 0.001000\n",
      "Epoch 400: Batch 12: loss = 0.000327, learning rate = 0.001000\n",
      "Epoch 400: Batch 13: loss = 0.000349, learning rate = 0.001000\n",
      "Epoch 400: Batch 14: loss = 0.000322, learning rate = 0.001000\n",
      "Epoch 400: Batch 15: loss = 0.000342, learning rate = 0.001000\n",
      "Epoch 400: Batch 16: loss = 0.000219, learning rate = 0.001000\n",
      "Epoch 400: Batch 17: loss = 0.000431, learning rate = 0.001000\n",
      "Epoch 400: Batch 18: loss = 0.000355, learning rate = 0.001000\n",
      "Epoch 400: Batch 19: loss = 0.000207, learning rate = 0.001000\n",
      "Epoch 400: Batch 20: loss = 0.000197, learning rate = 0.001000\n",
      "Epoch 400: Batch 21: loss = 0.000216, learning rate = 0.001000\n",
      "Epoch 400: Batch 22: loss = 0.000300, learning rate = 0.001000\n",
      "Epoch 400: Batch 23: loss = 0.000243, learning rate = 0.001000\n",
      "Epoch 400: Batch 24: loss = 0.000231, learning rate = 0.001000\n",
      "Epoch 400: Batch 25: loss = 0.000203, learning rate = 0.001000\n",
      "Epoch 400: Batch 26: loss = 0.000453, learning rate = 0.001000\n",
      "Epoch 400: Batch 27: loss = 0.000155, learning rate = 0.001000\n",
      "Epoch 400: Batch 28: loss = 0.000276, learning rate = 0.001000\n",
      "Epoch 400: Batch 29: loss = 0.000252, learning rate = 0.001000\n",
      "Epoch 400: Batch 30: loss = 0.000223, learning rate = 0.001000\n",
      "Epoch 400: Batch 31: loss = 0.000269, learning rate = 0.001000\n",
      "Epoch 450: Batch 0: loss = 0.000251, learning rate = 0.001000\n",
      "Epoch 450: Batch 1: loss = 0.000252, learning rate = 0.001000\n",
      "Epoch 450: Batch 2: loss = 0.000354, learning rate = 0.001000\n",
      "Epoch 450: Batch 3: loss = 0.000365, learning rate = 0.001000\n",
      "Epoch 450: Batch 4: loss = 0.000181, learning rate = 0.001000\n",
      "Epoch 450: Batch 5: loss = 0.000237, learning rate = 0.001000\n",
      "Epoch 450: Batch 6: loss = 0.000411, learning rate = 0.001000\n",
      "Epoch 450: Batch 7: loss = 0.000221, learning rate = 0.001000\n",
      "Epoch 450: Batch 8: loss = 0.000240, learning rate = 0.001000\n",
      "Epoch 450: Batch 9: loss = 0.000355, learning rate = 0.001000\n",
      "Epoch 450: Batch 10: loss = 0.000186, learning rate = 0.001000\n",
      "Epoch 450: Batch 11: loss = 0.000233, learning rate = 0.001000\n",
      "Epoch 450: Batch 12: loss = 0.000234, learning rate = 0.001000\n",
      "Epoch 450: Batch 13: loss = 0.000291, learning rate = 0.001000\n",
      "Epoch 450: Batch 14: loss = 0.000205, learning rate = 0.001000\n",
      "Epoch 450: Batch 15: loss = 0.000282, learning rate = 0.001000\n",
      "Epoch 450: Batch 16: loss = 0.000271, learning rate = 0.001000\n",
      "Epoch 450: Batch 17: loss = 0.000223, learning rate = 0.001000\n",
      "Epoch 450: Batch 18: loss = 0.000113, learning rate = 0.001000\n",
      "Epoch 450: Batch 19: loss = 0.000252, learning rate = 0.001000\n",
      "Epoch 450: Batch 20: loss = 0.000275, learning rate = 0.001000\n",
      "Epoch 450: Batch 21: loss = 0.000206, learning rate = 0.001000\n",
      "Epoch 450: Batch 22: loss = 0.000292, learning rate = 0.001000\n",
      "Epoch 450: Batch 23: loss = 0.000231, learning rate = 0.001000\n",
      "Epoch 450: Batch 24: loss = 0.000175, learning rate = 0.001000\n",
      "Epoch 450: Batch 25: loss = 0.000182, learning rate = 0.001000\n",
      "Epoch 450: Batch 26: loss = 0.000199, learning rate = 0.001000\n",
      "Epoch 450: Batch 27: loss = 0.000112, learning rate = 0.001000\n",
      "Epoch 450: Batch 28: loss = 0.000264, learning rate = 0.001000\n",
      "Epoch 450: Batch 29: loss = 0.000355, learning rate = 0.001000\n",
      "Epoch 450: Batch 30: loss = 0.000174, learning rate = 0.001000\n",
      "Epoch 450: Batch 31: loss = 0.000415, learning rate = 0.001000\n",
      "Epoch 500: Batch 0: loss = 0.000204, learning rate = 0.001000\n",
      "Epoch 500: Batch 1: loss = 0.000140, learning rate = 0.001000\n",
      "Epoch 500: Batch 2: loss = 0.000162, learning rate = 0.001000\n",
      "Epoch 500: Batch 3: loss = 0.000245, learning rate = 0.001000\n",
      "Epoch 500: Batch 4: loss = 0.000251, learning rate = 0.001000\n",
      "Epoch 500: Batch 5: loss = 0.000328, learning rate = 0.001000\n",
      "Epoch 500: Batch 6: loss = 0.000155, learning rate = 0.001000\n",
      "Epoch 500: Batch 7: loss = 0.000231, learning rate = 0.001000\n",
      "Epoch 500: Batch 8: loss = 0.000347, learning rate = 0.001000\n",
      "Epoch 500: Batch 9: loss = 0.000213, learning rate = 0.001000\n",
      "Epoch 500: Batch 10: loss = 0.000221, learning rate = 0.001000\n",
      "Epoch 500: Batch 11: loss = 0.000218, learning rate = 0.001000\n",
      "Epoch 500: Batch 12: loss = 0.000334, learning rate = 0.001000\n",
      "Epoch 500: Batch 13: loss = 0.000104, learning rate = 0.001000\n",
      "Epoch 500: Batch 14: loss = 0.000125, learning rate = 0.001000\n",
      "Epoch 500: Batch 15: loss = 0.000151, learning rate = 0.001000\n",
      "Epoch 500: Batch 16: loss = 0.000185, learning rate = 0.001000\n",
      "Epoch 500: Batch 17: loss = 0.000269, learning rate = 0.001000\n",
      "Epoch 500: Batch 18: loss = 0.000239, learning rate = 0.001000\n",
      "Epoch 500: Batch 19: loss = 0.000274, learning rate = 0.001000\n",
      "Epoch 500: Batch 20: loss = 0.000182, learning rate = 0.001000\n",
      "Epoch 500: Batch 21: loss = 0.000324, learning rate = 0.001000\n",
      "Epoch 500: Batch 22: loss = 0.000392, learning rate = 0.001000\n",
      "Epoch 500: Batch 23: loss = 0.000218, learning rate = 0.001000\n",
      "Epoch 500: Batch 24: loss = 0.000203, learning rate = 0.001000\n",
      "Epoch 500: Batch 25: loss = 0.000253, learning rate = 0.001000\n",
      "Epoch 500: Batch 26: loss = 0.000219, learning rate = 0.001000\n",
      "Epoch 500: Batch 27: loss = 0.000145, learning rate = 0.001000\n",
      "Epoch 500: Batch 28: loss = 0.000185, learning rate = 0.001000\n",
      "Epoch 500: Batch 29: loss = 0.000252, learning rate = 0.001000\n",
      "Epoch 500: Batch 30: loss = 0.000265, learning rate = 0.001000\n",
      "Epoch 500: Batch 31: loss = 0.000064, learning rate = 0.001000\n",
      "Epoch 550: Batch 0: loss = 0.000255, learning rate = 0.001000\n",
      "Epoch 550: Batch 1: loss = 0.000264, learning rate = 0.001000\n",
      "Epoch 550: Batch 2: loss = 0.000165, learning rate = 0.001000\n",
      "Epoch 550: Batch 3: loss = 0.000262, learning rate = 0.001000\n",
      "Epoch 550: Batch 4: loss = 0.000237, learning rate = 0.001000\n",
      "Epoch 550: Batch 5: loss = 0.000241, learning rate = 0.001000\n",
      "Epoch 550: Batch 6: loss = 0.000152, learning rate = 0.001000\n",
      "Epoch 550: Batch 7: loss = 0.000199, learning rate = 0.001000\n",
      "Epoch 550: Batch 8: loss = 0.000124, learning rate = 0.001000\n",
      "Epoch 550: Batch 9: loss = 0.000197, learning rate = 0.001000\n",
      "Epoch 550: Batch 10: loss = 0.000154, learning rate = 0.001000\n",
      "Epoch 550: Batch 11: loss = 0.000112, learning rate = 0.001000\n",
      "Epoch 550: Batch 12: loss = 0.000118, learning rate = 0.001000\n",
      "Epoch 550: Batch 13: loss = 0.000259, learning rate = 0.001000\n",
      "Epoch 550: Batch 14: loss = 0.000083, learning rate = 0.001000\n",
      "Epoch 550: Batch 15: loss = 0.000106, learning rate = 0.001000\n",
      "Epoch 550: Batch 16: loss = 0.000156, learning rate = 0.001000\n",
      "Epoch 550: Batch 17: loss = 0.000164, learning rate = 0.001000\n",
      "Epoch 550: Batch 18: loss = 0.000265, learning rate = 0.001000\n",
      "Epoch 550: Batch 19: loss = 0.000224, learning rate = 0.001000\n",
      "Epoch 550: Batch 20: loss = 0.000200, learning rate = 0.001000\n",
      "Epoch 550: Batch 21: loss = 0.000173, learning rate = 0.001000\n",
      "Epoch 550: Batch 22: loss = 0.000231, learning rate = 0.001000\n",
      "Epoch 550: Batch 23: loss = 0.000170, learning rate = 0.001000\n",
      "Epoch 550: Batch 24: loss = 0.000193, learning rate = 0.001000\n",
      "Epoch 550: Batch 25: loss = 0.000172, learning rate = 0.001000\n",
      "Epoch 550: Batch 26: loss = 0.000122, learning rate = 0.001000\n",
      "Epoch 550: Batch 27: loss = 0.000090, learning rate = 0.001000\n",
      "Epoch 550: Batch 28: loss = 0.000263, learning rate = 0.001000\n",
      "Epoch 550: Batch 29: loss = 0.000178, learning rate = 0.001000\n",
      "Epoch 550: Batch 30: loss = 0.000216, learning rate = 0.001000\n",
      "Epoch 550: Batch 31: loss = 0.000126, learning rate = 0.001000\n",
      "Epoch 600: Batch 0: loss = 0.000252, learning rate = 0.001000\n",
      "Epoch 600: Batch 1: loss = 0.000268, learning rate = 0.001000\n",
      "Epoch 600: Batch 2: loss = 0.000484, learning rate = 0.001000\n",
      "Epoch 600: Batch 3: loss = 0.000326, learning rate = 0.001000\n",
      "Epoch 600: Batch 4: loss = 0.000322, learning rate = 0.001000\n",
      "Epoch 600: Batch 5: loss = 0.000245, learning rate = 0.001000\n",
      "Epoch 600: Batch 6: loss = 0.000427, learning rate = 0.001000\n",
      "Epoch 600: Batch 7: loss = 0.000205, learning rate = 0.001000\n",
      "Epoch 600: Batch 8: loss = 0.000242, learning rate = 0.001000\n",
      "Epoch 600: Batch 9: loss = 0.000221, learning rate = 0.001000\n",
      "Epoch 600: Batch 10: loss = 0.000156, learning rate = 0.001000\n",
      "Epoch 600: Batch 11: loss = 0.000235, learning rate = 0.001000\n",
      "Epoch 600: Batch 12: loss = 0.000180, learning rate = 0.001000\n",
      "Epoch 600: Batch 13: loss = 0.000262, learning rate = 0.001000\n",
      "Epoch 600: Batch 14: loss = 0.000207, learning rate = 0.001000\n",
      "Epoch 600: Batch 15: loss = 0.000143, learning rate = 0.001000\n",
      "Epoch 600: Batch 16: loss = 0.000253, learning rate = 0.001000\n",
      "Epoch 600: Batch 17: loss = 0.000229, learning rate = 0.001000\n",
      "Epoch 600: Batch 18: loss = 0.000178, learning rate = 0.001000\n",
      "Epoch 600: Batch 19: loss = 0.000188, learning rate = 0.001000\n",
      "Epoch 600: Batch 20: loss = 0.000150, learning rate = 0.001000\n",
      "Epoch 600: Batch 21: loss = 0.000250, learning rate = 0.001000\n",
      "Epoch 600: Batch 22: loss = 0.000190, learning rate = 0.001000\n",
      "Epoch 600: Batch 23: loss = 0.000178, learning rate = 0.001000\n",
      "Epoch 600: Batch 24: loss = 0.000235, learning rate = 0.001000\n",
      "Epoch 600: Batch 25: loss = 0.000176, learning rate = 0.001000\n",
      "Epoch 600: Batch 26: loss = 0.000236, learning rate = 0.001000\n",
      "Epoch 600: Batch 27: loss = 0.000188, learning rate = 0.001000\n",
      "Epoch 600: Batch 28: loss = 0.000179, learning rate = 0.001000\n",
      "Epoch 600: Batch 29: loss = 0.000308, learning rate = 0.001000\n",
      "Epoch 600: Batch 30: loss = 0.000168, learning rate = 0.001000\n",
      "Epoch 600: Batch 31: loss = 0.000125, learning rate = 0.001000\n",
      "Epoch 650: Batch 0: loss = 0.000152, learning rate = 0.001000\n",
      "Epoch 650: Batch 1: loss = 0.000279, learning rate = 0.001000\n",
      "Epoch 650: Batch 2: loss = 0.000176, learning rate = 0.001000\n",
      "Epoch 650: Batch 3: loss = 0.000077, learning rate = 0.001000\n",
      "Epoch 650: Batch 4: loss = 0.000157, learning rate = 0.001000\n",
      "Epoch 650: Batch 5: loss = 0.000156, learning rate = 0.001000\n",
      "Epoch 650: Batch 6: loss = 0.000218, learning rate = 0.001000\n",
      "Epoch 650: Batch 7: loss = 0.000192, learning rate = 0.001000\n",
      "Epoch 650: Batch 8: loss = 0.000098, learning rate = 0.001000\n",
      "Epoch 650: Batch 9: loss = 0.000207, learning rate = 0.001000\n",
      "Epoch 650: Batch 10: loss = 0.000145, learning rate = 0.001000\n",
      "Epoch 650: Batch 11: loss = 0.000154, learning rate = 0.001000\n",
      "Epoch 650: Batch 12: loss = 0.000148, learning rate = 0.001000\n",
      "Epoch 650: Batch 13: loss = 0.000187, learning rate = 0.001000\n",
      "Epoch 650: Batch 14: loss = 0.000083, learning rate = 0.001000\n",
      "Epoch 650: Batch 15: loss = 0.000157, learning rate = 0.001000\n",
      "Epoch 650: Batch 16: loss = 0.000197, learning rate = 0.001000\n",
      "Epoch 650: Batch 17: loss = 0.000146, learning rate = 0.001000\n",
      "Epoch 650: Batch 18: loss = 0.000157, learning rate = 0.001000\n",
      "Epoch 650: Batch 19: loss = 0.000095, learning rate = 0.001000\n",
      "Epoch 650: Batch 20: loss = 0.000126, learning rate = 0.001000\n",
      "Epoch 650: Batch 21: loss = 0.000163, learning rate = 0.001000\n",
      "Epoch 650: Batch 22: loss = 0.000156, learning rate = 0.001000\n",
      "Epoch 650: Batch 23: loss = 0.000121, learning rate = 0.001000\n",
      "Epoch 650: Batch 24: loss = 0.000149, learning rate = 0.001000\n",
      "Epoch 650: Batch 25: loss = 0.000233, learning rate = 0.001000\n",
      "Epoch 650: Batch 26: loss = 0.000104, learning rate = 0.001000\n",
      "Epoch 650: Batch 27: loss = 0.000185, learning rate = 0.001000\n",
      "Epoch 650: Batch 28: loss = 0.000210, learning rate = 0.001000\n",
      "Epoch 650: Batch 29: loss = 0.000173, learning rate = 0.001000\n",
      "Epoch 650: Batch 30: loss = 0.000141, learning rate = 0.001000\n",
      "Epoch 650: Batch 31: loss = 0.000248, learning rate = 0.001000\n",
      "Epoch 700: Batch 0: loss = 0.000239, learning rate = 0.001000\n",
      "Epoch 700: Batch 1: loss = 0.000158, learning rate = 0.001000\n",
      "Epoch 700: Batch 2: loss = 0.000141, learning rate = 0.001000\n",
      "Epoch 700: Batch 3: loss = 0.000271, learning rate = 0.001000\n",
      "Epoch 700: Batch 4: loss = 0.000145, learning rate = 0.001000\n",
      "Epoch 700: Batch 5: loss = 0.000146, learning rate = 0.001000\n",
      "Epoch 700: Batch 6: loss = 0.000255, learning rate = 0.001000\n",
      "Epoch 700: Batch 7: loss = 0.000217, learning rate = 0.001000\n",
      "Epoch 700: Batch 8: loss = 0.000174, learning rate = 0.001000\n",
      "Epoch 700: Batch 9: loss = 0.000156, learning rate = 0.001000\n",
      "Epoch 700: Batch 10: loss = 0.000196, learning rate = 0.001000\n",
      "Epoch 700: Batch 11: loss = 0.000194, learning rate = 0.001000\n",
      "Epoch 700: Batch 12: loss = 0.000188, learning rate = 0.001000\n",
      "Epoch 700: Batch 13: loss = 0.000122, learning rate = 0.001000\n",
      "Epoch 700: Batch 14: loss = 0.000226, learning rate = 0.001000\n",
      "Epoch 700: Batch 15: loss = 0.000175, learning rate = 0.001000\n",
      "Epoch 700: Batch 16: loss = 0.000202, learning rate = 0.001000\n",
      "Epoch 700: Batch 17: loss = 0.000144, learning rate = 0.001000\n",
      "Epoch 700: Batch 18: loss = 0.000208, learning rate = 0.001000\n",
      "Epoch 700: Batch 19: loss = 0.000133, learning rate = 0.001000\n",
      "Epoch 700: Batch 20: loss = 0.000161, learning rate = 0.001000\n",
      "Epoch 700: Batch 21: loss = 0.000161, learning rate = 0.001000\n",
      "Epoch 700: Batch 22: loss = 0.000140, learning rate = 0.001000\n",
      "Epoch 700: Batch 23: loss = 0.000181, learning rate = 0.001000\n",
      "Epoch 700: Batch 24: loss = 0.000180, learning rate = 0.001000\n",
      "Epoch 700: Batch 25: loss = 0.000160, learning rate = 0.001000\n",
      "Epoch 700: Batch 26: loss = 0.000150, learning rate = 0.001000\n",
      "Epoch 700: Batch 27: loss = 0.000166, learning rate = 0.001000\n",
      "Epoch 700: Batch 28: loss = 0.000188, learning rate = 0.001000\n",
      "Epoch 700: Batch 29: loss = 0.000161, learning rate = 0.001000\n",
      "Epoch 700: Batch 30: loss = 0.000257, learning rate = 0.001000\n",
      "Epoch 700: Batch 31: loss = 0.000183, learning rate = 0.001000\n",
      "Epoch 750: Batch 0: loss = 0.000097, learning rate = 0.001000\n",
      "Epoch 750: Batch 1: loss = 0.000207, learning rate = 0.001000\n",
      "Epoch 750: Batch 2: loss = 0.000158, learning rate = 0.001000\n",
      "Epoch 750: Batch 3: loss = 0.000053, learning rate = 0.001000\n",
      "Epoch 750: Batch 4: loss = 0.000145, learning rate = 0.001000\n",
      "Epoch 750: Batch 5: loss = 0.000126, learning rate = 0.001000\n",
      "Epoch 750: Batch 6: loss = 0.000158, learning rate = 0.001000\n",
      "Epoch 750: Batch 7: loss = 0.000171, learning rate = 0.001000\n",
      "Epoch 750: Batch 8: loss = 0.000156, learning rate = 0.001000\n",
      "Epoch 750: Batch 9: loss = 0.000146, learning rate = 0.001000\n",
      "Epoch 750: Batch 10: loss = 0.000108, learning rate = 0.001000\n",
      "Epoch 750: Batch 11: loss = 0.000169, learning rate = 0.001000\n",
      "Epoch 750: Batch 12: loss = 0.000107, learning rate = 0.001000\n",
      "Epoch 750: Batch 13: loss = 0.000157, learning rate = 0.001000\n",
      "Epoch 750: Batch 14: loss = 0.000177, learning rate = 0.001000\n",
      "Epoch 750: Batch 15: loss = 0.000155, learning rate = 0.001000\n",
      "Epoch 750: Batch 16: loss = 0.000201, learning rate = 0.001000\n",
      "Epoch 750: Batch 17: loss = 0.000183, learning rate = 0.001000\n",
      "Epoch 750: Batch 18: loss = 0.000141, learning rate = 0.001000\n",
      "Epoch 750: Batch 19: loss = 0.000137, learning rate = 0.001000\n",
      "Epoch 750: Batch 20: loss = 0.000114, learning rate = 0.001000\n",
      "Epoch 750: Batch 21: loss = 0.000127, learning rate = 0.001000\n",
      "Epoch 750: Batch 22: loss = 0.000107, learning rate = 0.001000\n",
      "Epoch 750: Batch 23: loss = 0.000124, learning rate = 0.001000\n",
      "Epoch 750: Batch 24: loss = 0.000188, learning rate = 0.001000\n",
      "Epoch 750: Batch 25: loss = 0.000152, learning rate = 0.001000\n",
      "Epoch 750: Batch 26: loss = 0.000204, learning rate = 0.001000\n",
      "Epoch 750: Batch 27: loss = 0.000128, learning rate = 0.001000\n",
      "Epoch 750: Batch 28: loss = 0.000139, learning rate = 0.001000\n",
      "Epoch 750: Batch 29: loss = 0.000098, learning rate = 0.001000\n",
      "Epoch 750: Batch 30: loss = 0.000126, learning rate = 0.001000\n",
      "Epoch 750: Batch 31: loss = 0.000116, learning rate = 0.001000\n",
      "Epoch 800: Batch 0: loss = 0.000173, learning rate = 0.001000\n",
      "Epoch 800: Batch 1: loss = 0.000203, learning rate = 0.001000\n",
      "Epoch 800: Batch 2: loss = 0.000234, learning rate = 0.001000\n",
      "Epoch 800: Batch 3: loss = 0.000188, learning rate = 0.001000\n",
      "Epoch 800: Batch 4: loss = 0.000207, learning rate = 0.001000\n",
      "Epoch 800: Batch 5: loss = 0.000183, learning rate = 0.001000\n",
      "Epoch 800: Batch 6: loss = 0.000198, learning rate = 0.001000\n",
      "Epoch 800: Batch 7: loss = 0.000296, learning rate = 0.001000\n",
      "Epoch 800: Batch 8: loss = 0.000192, learning rate = 0.001000\n",
      "Epoch 800: Batch 9: loss = 0.000293, learning rate = 0.001000\n",
      "Epoch 800: Batch 10: loss = 0.000133, learning rate = 0.001000\n",
      "Epoch 800: Batch 11: loss = 0.000267, learning rate = 0.001000\n",
      "Epoch 800: Batch 12: loss = 0.000218, learning rate = 0.001000\n",
      "Epoch 800: Batch 13: loss = 0.000237, learning rate = 0.001000\n",
      "Epoch 800: Batch 14: loss = 0.000244, learning rate = 0.001000\n",
      "Epoch 800: Batch 15: loss = 0.000236, learning rate = 0.001000\n",
      "Epoch 800: Batch 16: loss = 0.000169, learning rate = 0.001000\n",
      "Epoch 800: Batch 17: loss = 0.000187, learning rate = 0.001000\n",
      "Epoch 800: Batch 18: loss = 0.000307, learning rate = 0.001000\n",
      "Epoch 800: Batch 19: loss = 0.000179, learning rate = 0.001000\n",
      "Epoch 800: Batch 20: loss = 0.000115, learning rate = 0.001000\n",
      "Epoch 800: Batch 21: loss = 0.000248, learning rate = 0.001000\n",
      "Epoch 800: Batch 22: loss = 0.000127, learning rate = 0.001000\n",
      "Epoch 800: Batch 23: loss = 0.000183, learning rate = 0.001000\n",
      "Epoch 800: Batch 24: loss = 0.000221, learning rate = 0.001000\n",
      "Epoch 800: Batch 25: loss = 0.000167, learning rate = 0.001000\n",
      "Epoch 800: Batch 26: loss = 0.000233, learning rate = 0.001000\n",
      "Epoch 800: Batch 27: loss = 0.000160, learning rate = 0.001000\n",
      "Epoch 800: Batch 28: loss = 0.000091, learning rate = 0.001000\n",
      "Epoch 800: Batch 29: loss = 0.000153, learning rate = 0.001000\n",
      "Epoch 800: Batch 30: loss = 0.000167, learning rate = 0.001000\n",
      "Epoch 800: Batch 31: loss = 0.000065, learning rate = 0.001000\n",
      "Epoch 850: Batch 0: loss = 0.000099, learning rate = 0.001000\n",
      "Epoch 850: Batch 1: loss = 0.000128, learning rate = 0.001000\n",
      "Epoch 850: Batch 2: loss = 0.000110, learning rate = 0.001000\n",
      "Epoch 850: Batch 3: loss = 0.000102, learning rate = 0.001000\n",
      "Epoch 850: Batch 4: loss = 0.000172, learning rate = 0.001000\n",
      "Epoch 850: Batch 5: loss = 0.000087, learning rate = 0.001000\n",
      "Epoch 850: Batch 6: loss = 0.000118, learning rate = 0.001000\n",
      "Epoch 850: Batch 7: loss = 0.000133, learning rate = 0.001000\n",
      "Epoch 850: Batch 8: loss = 0.000106, learning rate = 0.001000\n",
      "Epoch 850: Batch 9: loss = 0.000106, learning rate = 0.001000\n",
      "Epoch 850: Batch 10: loss = 0.000094, learning rate = 0.001000\n",
      "Epoch 850: Batch 11: loss = 0.000157, learning rate = 0.001000\n",
      "Epoch 850: Batch 12: loss = 0.000126, learning rate = 0.001000\n",
      "Epoch 850: Batch 13: loss = 0.000118, learning rate = 0.001000\n",
      "Epoch 850: Batch 14: loss = 0.000142, learning rate = 0.001000\n",
      "Epoch 850: Batch 15: loss = 0.000110, learning rate = 0.001000\n",
      "Epoch 850: Batch 16: loss = 0.000125, learning rate = 0.001000\n",
      "Epoch 850: Batch 17: loss = 0.000118, learning rate = 0.001000\n",
      "Epoch 850: Batch 18: loss = 0.000121, learning rate = 0.001000\n",
      "Epoch 850: Batch 19: loss = 0.000068, learning rate = 0.001000\n",
      "Epoch 850: Batch 20: loss = 0.000132, learning rate = 0.001000\n",
      "Epoch 850: Batch 21: loss = 0.000132, learning rate = 0.001000\n",
      "Epoch 850: Batch 22: loss = 0.000128, learning rate = 0.001000\n",
      "Epoch 850: Batch 23: loss = 0.000163, learning rate = 0.001000\n",
      "Epoch 850: Batch 24: loss = 0.000241, learning rate = 0.001000\n",
      "Epoch 850: Batch 25: loss = 0.000126, learning rate = 0.001000\n",
      "Epoch 850: Batch 26: loss = 0.000116, learning rate = 0.001000\n",
      "Epoch 850: Batch 27: loss = 0.000088, learning rate = 0.001000\n",
      "Epoch 850: Batch 28: loss = 0.000139, learning rate = 0.001000\n",
      "Epoch 850: Batch 29: loss = 0.000114, learning rate = 0.001000\n",
      "Epoch 850: Batch 30: loss = 0.000156, learning rate = 0.001000\n",
      "Epoch 850: Batch 31: loss = 0.000191, learning rate = 0.001000\n",
      "Epoch 900: Batch 0: loss = 0.000107, learning rate = 0.001000\n",
      "Epoch 900: Batch 1: loss = 0.000165, learning rate = 0.001000\n",
      "Epoch 900: Batch 2: loss = 0.000145, learning rate = 0.001000\n",
      "Epoch 900: Batch 3: loss = 0.000214, learning rate = 0.001000\n",
      "Epoch 900: Batch 4: loss = 0.000122, learning rate = 0.001000\n",
      "Epoch 900: Batch 5: loss = 0.000141, learning rate = 0.001000\n",
      "Epoch 900: Batch 6: loss = 0.000119, learning rate = 0.001000\n",
      "Epoch 900: Batch 7: loss = 0.000113, learning rate = 0.001000\n",
      "Epoch 900: Batch 8: loss = 0.000161, learning rate = 0.001000\n",
      "Epoch 900: Batch 9: loss = 0.000126, learning rate = 0.001000\n",
      "Epoch 900: Batch 10: loss = 0.000100, learning rate = 0.001000\n",
      "Epoch 900: Batch 11: loss = 0.000164, learning rate = 0.001000\n",
      "Epoch 900: Batch 12: loss = 0.000120, learning rate = 0.001000\n",
      "Epoch 900: Batch 13: loss = 0.000157, learning rate = 0.001000\n",
      "Epoch 900: Batch 14: loss = 0.000094, learning rate = 0.001000\n",
      "Epoch 900: Batch 15: loss = 0.000127, learning rate = 0.001000\n",
      "Epoch 900: Batch 16: loss = 0.000126, learning rate = 0.001000\n",
      "Epoch 900: Batch 17: loss = 0.000155, learning rate = 0.001000\n",
      "Epoch 900: Batch 18: loss = 0.000113, learning rate = 0.001000\n",
      "Epoch 900: Batch 19: loss = 0.000135, learning rate = 0.001000\n",
      "Epoch 900: Batch 20: loss = 0.000088, learning rate = 0.001000\n",
      "Epoch 900: Batch 21: loss = 0.000121, learning rate = 0.001000\n",
      "Epoch 900: Batch 22: loss = 0.000107, learning rate = 0.001000\n",
      "Epoch 900: Batch 23: loss = 0.000156, learning rate = 0.001000\n",
      "Epoch 900: Batch 24: loss = 0.000101, learning rate = 0.001000\n",
      "Epoch 900: Batch 25: loss = 0.000103, learning rate = 0.001000\n",
      "Epoch 900: Batch 26: loss = 0.000108, learning rate = 0.001000\n",
      "Epoch 900: Batch 27: loss = 0.000082, learning rate = 0.001000\n",
      "Epoch 900: Batch 28: loss = 0.000118, learning rate = 0.001000\n",
      "Epoch 900: Batch 29: loss = 0.000160, learning rate = 0.001000\n",
      "Epoch 900: Batch 30: loss = 0.000089, learning rate = 0.001000\n",
      "Epoch 900: Batch 31: loss = 0.000090, learning rate = 0.001000\n",
      "Epoch 950: Batch 0: loss = 0.000207, learning rate = 0.001000\n",
      "Epoch 950: Batch 1: loss = 0.000086, learning rate = 0.001000\n",
      "Epoch 950: Batch 2: loss = 0.000120, learning rate = 0.001000\n",
      "Epoch 950: Batch 3: loss = 0.000067, learning rate = 0.001000\n",
      "Epoch 950: Batch 4: loss = 0.000149, learning rate = 0.001000\n",
      "Epoch 950: Batch 5: loss = 0.000081, learning rate = 0.001000\n",
      "Epoch 950: Batch 6: loss = 0.000187, learning rate = 0.001000\n",
      "Epoch 950: Batch 7: loss = 0.000104, learning rate = 0.001000\n",
      "Epoch 950: Batch 8: loss = 0.000133, learning rate = 0.001000\n",
      "Epoch 950: Batch 9: loss = 0.000184, learning rate = 0.001000\n",
      "Epoch 950: Batch 10: loss = 0.000120, learning rate = 0.001000\n",
      "Epoch 950: Batch 11: loss = 0.000096, learning rate = 0.001000\n",
      "Epoch 950: Batch 12: loss = 0.000107, learning rate = 0.001000\n",
      "Epoch 950: Batch 13: loss = 0.000138, learning rate = 0.001000\n",
      "Epoch 950: Batch 14: loss = 0.000077, learning rate = 0.001000\n",
      "Epoch 950: Batch 15: loss = 0.000092, learning rate = 0.001000\n",
      "Epoch 950: Batch 16: loss = 0.000145, learning rate = 0.001000\n",
      "Epoch 950: Batch 17: loss = 0.000071, learning rate = 0.001000\n",
      "Epoch 950: Batch 18: loss = 0.000130, learning rate = 0.001000\n",
      "Epoch 950: Batch 19: loss = 0.000108, learning rate = 0.001000\n",
      "Epoch 950: Batch 20: loss = 0.000160, learning rate = 0.001000\n",
      "Epoch 950: Batch 21: loss = 0.000127, learning rate = 0.001000\n",
      "Epoch 950: Batch 22: loss = 0.000124, learning rate = 0.001000\n",
      "Epoch 950: Batch 23: loss = 0.000077, learning rate = 0.001000\n",
      "Epoch 950: Batch 24: loss = 0.000236, learning rate = 0.001000\n",
      "Epoch 950: Batch 25: loss = 0.000136, learning rate = 0.001000\n",
      "Epoch 950: Batch 26: loss = 0.000089, learning rate = 0.001000\n",
      "Epoch 950: Batch 27: loss = 0.000094, learning rate = 0.001000\n",
      "Epoch 950: Batch 28: loss = 0.000085, learning rate = 0.001000\n",
      "Epoch 950: Batch 29: loss = 0.000120, learning rate = 0.001000\n",
      "Epoch 950: Batch 30: loss = 0.000111, learning rate = 0.001000\n",
      "Epoch 950: Batch 31: loss = 0.000099, learning rate = 0.001000\n",
      "Epoch 1000: Batch 0: loss = 0.000077, learning rate = 0.001000\n",
      "Epoch 1000: Batch 1: loss = 0.000122, learning rate = 0.001000\n",
      "Epoch 1000: Batch 2: loss = 0.000067, learning rate = 0.001000\n",
      "Epoch 1000: Batch 3: loss = 0.000155, learning rate = 0.001000\n",
      "Epoch 1000: Batch 4: loss = 0.000090, learning rate = 0.001000\n",
      "Epoch 1000: Batch 5: loss = 0.000077, learning rate = 0.001000\n",
      "Epoch 1000: Batch 6: loss = 0.000157, learning rate = 0.001000\n",
      "Epoch 1000: Batch 7: loss = 0.000093, learning rate = 0.001000\n",
      "Epoch 1000: Batch 8: loss = 0.000178, learning rate = 0.001000\n",
      "Epoch 1000: Batch 9: loss = 0.000095, learning rate = 0.001000\n",
      "Epoch 1000: Batch 10: loss = 0.000111, learning rate = 0.001000\n",
      "Epoch 1000: Batch 11: loss = 0.000092, learning rate = 0.001000\n",
      "Epoch 1000: Batch 12: loss = 0.000078, learning rate = 0.001000\n",
      "Epoch 1000: Batch 13: loss = 0.000096, learning rate = 0.001000\n",
      "Epoch 1000: Batch 14: loss = 0.000203, learning rate = 0.001000\n",
      "Epoch 1000: Batch 15: loss = 0.000118, learning rate = 0.001000\n",
      "Epoch 1000: Batch 16: loss = 0.000102, learning rate = 0.001000\n",
      "Epoch 1000: Batch 17: loss = 0.000118, learning rate = 0.001000\n",
      "Epoch 1000: Batch 18: loss = 0.000075, learning rate = 0.001000\n",
      "Epoch 1000: Batch 19: loss = 0.000117, learning rate = 0.001000\n",
      "Epoch 1000: Batch 20: loss = 0.000119, learning rate = 0.001000\n",
      "Epoch 1000: Batch 21: loss = 0.000172, learning rate = 0.001000\n",
      "Epoch 1000: Batch 22: loss = 0.000104, learning rate = 0.001000\n",
      "Epoch 1000: Batch 23: loss = 0.000089, learning rate = 0.001000\n",
      "Epoch 1000: Batch 24: loss = 0.000087, learning rate = 0.001000\n",
      "Epoch 1000: Batch 25: loss = 0.000091, learning rate = 0.001000\n",
      "Epoch 1000: Batch 26: loss = 0.000102, learning rate = 0.001000\n",
      "Epoch 1000: Batch 27: loss = 0.000074, learning rate = 0.001000\n",
      "Epoch 1000: Batch 28: loss = 0.000116, learning rate = 0.001000\n",
      "Epoch 1000: Batch 29: loss = 0.000121, learning rate = 0.001000\n",
      "Epoch 1000: Batch 30: loss = 0.000111, learning rate = 0.001000\n",
      "Epoch 1000: Batch 31: loss = 0.000105, learning rate = 0.001000\n",
      "Epoch 1050: Batch 0: loss = 0.000084, learning rate = 0.001000\n",
      "Epoch 1050: Batch 1: loss = 0.000091, learning rate = 0.001000\n",
      "Epoch 1050: Batch 2: loss = 0.000089, learning rate = 0.001000\n",
      "Epoch 1050: Batch 3: loss = 0.000077, learning rate = 0.001000\n",
      "Epoch 1050: Batch 4: loss = 0.000117, learning rate = 0.001000\n",
      "Epoch 1050: Batch 5: loss = 0.000096, learning rate = 0.001000\n",
      "Epoch 1050: Batch 6: loss = 0.000069, learning rate = 0.001000\n",
      "Epoch 1050: Batch 7: loss = 0.000068, learning rate = 0.001000\n",
      "Epoch 1050: Batch 8: loss = 0.000122, learning rate = 0.001000\n",
      "Epoch 1050: Batch 9: loss = 0.000046, learning rate = 0.001000\n",
      "Epoch 1050: Batch 10: loss = 0.000146, learning rate = 0.001000\n",
      "Epoch 1050: Batch 11: loss = 0.000085, learning rate = 0.001000\n",
      "Epoch 1050: Batch 12: loss = 0.000094, learning rate = 0.001000\n",
      "Epoch 1050: Batch 13: loss = 0.000057, learning rate = 0.001000\n",
      "Epoch 1050: Batch 14: loss = 0.000075, learning rate = 0.001000\n",
      "Epoch 1050: Batch 15: loss = 0.000059, learning rate = 0.001000\n",
      "Epoch 1050: Batch 16: loss = 0.000117, learning rate = 0.001000\n",
      "Epoch 1050: Batch 17: loss = 0.000068, learning rate = 0.001000\n",
      "Epoch 1050: Batch 18: loss = 0.000113, learning rate = 0.001000\n",
      "Epoch 1050: Batch 19: loss = 0.000180, learning rate = 0.001000\n",
      "Epoch 1050: Batch 20: loss = 0.000057, learning rate = 0.001000\n",
      "Epoch 1050: Batch 21: loss = 0.000111, learning rate = 0.001000\n",
      "Epoch 1050: Batch 22: loss = 0.000088, learning rate = 0.001000\n",
      "Epoch 1050: Batch 23: loss = 0.000131, learning rate = 0.001000\n",
      "Epoch 1050: Batch 24: loss = 0.000080, learning rate = 0.001000\n",
      "Epoch 1050: Batch 25: loss = 0.000121, learning rate = 0.001000\n",
      "Epoch 1050: Batch 26: loss = 0.000090, learning rate = 0.001000\n",
      "Epoch 1050: Batch 27: loss = 0.000057, learning rate = 0.001000\n",
      "Epoch 1050: Batch 28: loss = 0.000139, learning rate = 0.001000\n",
      "Epoch 1050: Batch 29: loss = 0.000162, learning rate = 0.001000\n",
      "Epoch 1050: Batch 30: loss = 0.000080, learning rate = 0.001000\n",
      "Epoch 1050: Batch 31: loss = 0.000064, learning rate = 0.001000\n",
      "Epoch 1100: Batch 0: loss = 0.000092, learning rate = 0.001000\n",
      "Epoch 1100: Batch 1: loss = 0.000089, learning rate = 0.001000\n",
      "Epoch 1100: Batch 2: loss = 0.000123, learning rate = 0.001000\n",
      "Epoch 1100: Batch 3: loss = 0.000161, learning rate = 0.001000\n",
      "Epoch 1100: Batch 4: loss = 0.000111, learning rate = 0.001000\n",
      "Epoch 1100: Batch 5: loss = 0.000130, learning rate = 0.001000\n",
      "Epoch 1100: Batch 6: loss = 0.000098, learning rate = 0.001000\n",
      "Epoch 1100: Batch 7: loss = 0.000110, learning rate = 0.001000\n",
      "Epoch 1100: Batch 8: loss = 0.000123, learning rate = 0.001000\n",
      "Epoch 1100: Batch 9: loss = 0.000102, learning rate = 0.001000\n",
      "Epoch 1100: Batch 10: loss = 0.000086, learning rate = 0.001000\n",
      "Epoch 1100: Batch 11: loss = 0.000101, learning rate = 0.001000\n",
      "Epoch 1100: Batch 12: loss = 0.000104, learning rate = 0.001000\n",
      "Epoch 1100: Batch 13: loss = 0.000071, learning rate = 0.001000\n",
      "Epoch 1100: Batch 14: loss = 0.000135, learning rate = 0.001000\n",
      "Epoch 1100: Batch 15: loss = 0.000088, learning rate = 0.001000\n",
      "Epoch 1100: Batch 16: loss = 0.000138, learning rate = 0.001000\n",
      "Epoch 1100: Batch 17: loss = 0.000115, learning rate = 0.001000\n",
      "Epoch 1100: Batch 18: loss = 0.000110, learning rate = 0.001000\n",
      "Epoch 1100: Batch 19: loss = 0.000091, learning rate = 0.001000\n",
      "Epoch 1100: Batch 20: loss = 0.000088, learning rate = 0.001000\n",
      "Epoch 1100: Batch 21: loss = 0.000052, learning rate = 0.001000\n",
      "Epoch 1100: Batch 22: loss = 0.000134, learning rate = 0.001000\n",
      "Epoch 1100: Batch 23: loss = 0.000071, learning rate = 0.001000\n",
      "Epoch 1100: Batch 24: loss = 0.000088, learning rate = 0.001000\n",
      "Epoch 1100: Batch 25: loss = 0.000087, learning rate = 0.001000\n",
      "Epoch 1100: Batch 26: loss = 0.000140, learning rate = 0.001000\n",
      "Epoch 1100: Batch 27: loss = 0.000058, learning rate = 0.001000\n",
      "Epoch 1100: Batch 28: loss = 0.000071, learning rate = 0.001000\n",
      "Epoch 1100: Batch 29: loss = 0.000098, learning rate = 0.001000\n",
      "Epoch 1100: Batch 30: loss = 0.000123, learning rate = 0.001000\n",
      "Epoch 1100: Batch 31: loss = 0.000040, learning rate = 0.001000\n",
      "Epoch 1150: Batch 0: loss = 0.000045, learning rate = 0.001000\n",
      "Epoch 1150: Batch 1: loss = 0.000085, learning rate = 0.001000\n",
      "Epoch 1150: Batch 2: loss = 0.000055, learning rate = 0.001000\n",
      "Epoch 1150: Batch 3: loss = 0.000085, learning rate = 0.001000\n",
      "Epoch 1150: Batch 4: loss = 0.000065, learning rate = 0.001000\n",
      "Epoch 1150: Batch 5: loss = 0.000125, learning rate = 0.001000\n",
      "Epoch 1150: Batch 6: loss = 0.000057, learning rate = 0.001000\n",
      "Epoch 1150: Batch 7: loss = 0.000082, learning rate = 0.001000\n",
      "Epoch 1150: Batch 8: loss = 0.000135, learning rate = 0.001000\n",
      "Epoch 1150: Batch 9: loss = 0.000082, learning rate = 0.001000\n",
      "Epoch 1150: Batch 10: loss = 0.000109, learning rate = 0.001000\n",
      "Epoch 1150: Batch 11: loss = 0.000075, learning rate = 0.001000\n",
      "Epoch 1150: Batch 12: loss = 0.000089, learning rate = 0.001000\n",
      "Epoch 1150: Batch 13: loss = 0.000058, learning rate = 0.001000\n",
      "Epoch 1150: Batch 14: loss = 0.000109, learning rate = 0.001000\n",
      "Epoch 1150: Batch 15: loss = 0.000108, learning rate = 0.001000\n",
      "Epoch 1150: Batch 16: loss = 0.000061, learning rate = 0.001000\n",
      "Epoch 1150: Batch 17: loss = 0.000070, learning rate = 0.001000\n",
      "Epoch 1150: Batch 18: loss = 0.000081, learning rate = 0.001000\n",
      "Epoch 1150: Batch 19: loss = 0.000072, learning rate = 0.001000\n",
      "Epoch 1150: Batch 20: loss = 0.000079, learning rate = 0.001000\n",
      "Epoch 1150: Batch 21: loss = 0.000113, learning rate = 0.001000\n",
      "Epoch 1150: Batch 22: loss = 0.000153, learning rate = 0.001000\n",
      "Epoch 1150: Batch 23: loss = 0.000060, learning rate = 0.001000\n",
      "Epoch 1150: Batch 24: loss = 0.000094, learning rate = 0.001000\n",
      "Epoch 1150: Batch 25: loss = 0.000062, learning rate = 0.001000\n",
      "Epoch 1150: Batch 26: loss = 0.000083, learning rate = 0.001000\n",
      "Epoch 1150: Batch 27: loss = 0.000073, learning rate = 0.001000\n",
      "Epoch 1150: Batch 28: loss = 0.000143, learning rate = 0.001000\n",
      "Epoch 1150: Batch 29: loss = 0.000084, learning rate = 0.001000\n",
      "Epoch 1150: Batch 30: loss = 0.000149, learning rate = 0.001000\n",
      "Epoch 1150: Batch 31: loss = 0.000054, learning rate = 0.001000\n",
      "Epoch 1200: Batch 0: loss = 0.000082, learning rate = 0.001000\n",
      "Epoch 1200: Batch 1: loss = 0.000088, learning rate = 0.001000\n",
      "Epoch 1200: Batch 2: loss = 0.000089, learning rate = 0.001000\n",
      "Epoch 1200: Batch 3: loss = 0.000050, learning rate = 0.001000\n",
      "Epoch 1200: Batch 4: loss = 0.000069, learning rate = 0.001000\n",
      "Epoch 1200: Batch 5: loss = 0.000068, learning rate = 0.001000\n",
      "Epoch 1200: Batch 6: loss = 0.000101, learning rate = 0.001000\n",
      "Epoch 1200: Batch 7: loss = 0.000132, learning rate = 0.001000\n",
      "Epoch 1200: Batch 8: loss = 0.000160, learning rate = 0.001000\n",
      "Epoch 1200: Batch 9: loss = 0.000074, learning rate = 0.001000\n",
      "Epoch 1200: Batch 10: loss = 0.000076, learning rate = 0.001000\n",
      "Epoch 1200: Batch 11: loss = 0.000092, learning rate = 0.001000\n",
      "Epoch 1200: Batch 12: loss = 0.000130, learning rate = 0.001000\n",
      "Epoch 1200: Batch 13: loss = 0.000065, learning rate = 0.001000\n",
      "Epoch 1200: Batch 14: loss = 0.000111, learning rate = 0.001000\n",
      "Epoch 1200: Batch 15: loss = 0.000092, learning rate = 0.001000\n",
      "Epoch 1200: Batch 16: loss = 0.000071, learning rate = 0.001000\n",
      "Epoch 1200: Batch 17: loss = 0.000071, learning rate = 0.001000\n",
      "Epoch 1200: Batch 18: loss = 0.000089, learning rate = 0.001000\n",
      "Epoch 1200: Batch 19: loss = 0.000039, learning rate = 0.001000\n",
      "Epoch 1200: Batch 20: loss = 0.000080, learning rate = 0.001000\n",
      "Epoch 1200: Batch 21: loss = 0.000080, learning rate = 0.001000\n",
      "Epoch 1200: Batch 22: loss = 0.000088, learning rate = 0.001000\n",
      "Epoch 1200: Batch 23: loss = 0.000049, learning rate = 0.001000\n",
      "Epoch 1200: Batch 24: loss = 0.000110, learning rate = 0.001000\n",
      "Epoch 1200: Batch 25: loss = 0.000103, learning rate = 0.001000\n",
      "Epoch 1200: Batch 26: loss = 0.000105, learning rate = 0.001000\n",
      "Epoch 1200: Batch 27: loss = 0.000102, learning rate = 0.001000\n",
      "Epoch 1200: Batch 28: loss = 0.000070, learning rate = 0.001000\n",
      "Epoch 1200: Batch 29: loss = 0.000079, learning rate = 0.001000\n",
      "Epoch 1200: Batch 30: loss = 0.000099, learning rate = 0.001000\n",
      "Epoch 1200: Batch 31: loss = 0.000128, learning rate = 0.001000\n",
      "Epoch 1250: Batch 0: loss = 0.000120, learning rate = 0.001000\n",
      "Epoch 1250: Batch 1: loss = 0.000129, learning rate = 0.001000\n",
      "Epoch 1250: Batch 2: loss = 0.000093, learning rate = 0.001000\n",
      "Epoch 1250: Batch 3: loss = 0.000048, learning rate = 0.001000\n",
      "Epoch 1250: Batch 4: loss = 0.000062, learning rate = 0.001000\n",
      "Epoch 1250: Batch 5: loss = 0.000055, learning rate = 0.001000\n",
      "Epoch 1250: Batch 6: loss = 0.000101, learning rate = 0.001000\n",
      "Epoch 1250: Batch 7: loss = 0.000075, learning rate = 0.001000\n",
      "Epoch 1250: Batch 8: loss = 0.000054, learning rate = 0.001000\n",
      "Epoch 1250: Batch 9: loss = 0.000045, learning rate = 0.001000\n",
      "Epoch 1250: Batch 10: loss = 0.000105, learning rate = 0.001000\n",
      "Epoch 1250: Batch 11: loss = 0.000063, learning rate = 0.001000\n",
      "Epoch 1250: Batch 12: loss = 0.000125, learning rate = 0.001000\n",
      "Epoch 1250: Batch 13: loss = 0.000099, learning rate = 0.001000\n",
      "Epoch 1250: Batch 14: loss = 0.000099, learning rate = 0.001000\n",
      "Epoch 1250: Batch 15: loss = 0.000067, learning rate = 0.001000\n",
      "Epoch 1250: Batch 16: loss = 0.000056, learning rate = 0.001000\n",
      "Epoch 1250: Batch 17: loss = 0.000183, learning rate = 0.001000\n",
      "Epoch 1250: Batch 18: loss = 0.000113, learning rate = 0.001000\n",
      "Epoch 1250: Batch 19: loss = 0.000121, learning rate = 0.001000\n",
      "Epoch 1250: Batch 20: loss = 0.000124, learning rate = 0.001000\n",
      "Epoch 1250: Batch 21: loss = 0.000068, learning rate = 0.001000\n",
      "Epoch 1250: Batch 22: loss = 0.000101, learning rate = 0.001000\n",
      "Epoch 1250: Batch 23: loss = 0.000106, learning rate = 0.001000\n",
      "Epoch 1250: Batch 24: loss = 0.000093, learning rate = 0.001000\n",
      "Epoch 1250: Batch 25: loss = 0.000078, learning rate = 0.001000\n",
      "Epoch 1250: Batch 26: loss = 0.000047, learning rate = 0.001000\n",
      "Epoch 1250: Batch 27: loss = 0.000097, learning rate = 0.001000\n",
      "Epoch 1250: Batch 28: loss = 0.000087, learning rate = 0.001000\n",
      "Epoch 1250: Batch 29: loss = 0.000123, learning rate = 0.001000\n",
      "Epoch 1250: Batch 30: loss = 0.000092, learning rate = 0.001000\n",
      "Epoch 1250: Batch 31: loss = 0.000060, learning rate = 0.001000\n",
      "Epoch 1300: Batch 0: loss = 0.000119, learning rate = 0.001000\n",
      "Epoch 1300: Batch 1: loss = 0.000077, learning rate = 0.001000\n",
      "Epoch 1300: Batch 2: loss = 0.000098, learning rate = 0.001000\n",
      "Epoch 1300: Batch 3: loss = 0.000136, learning rate = 0.001000\n",
      "Epoch 1300: Batch 4: loss = 0.000065, learning rate = 0.001000\n",
      "Epoch 1300: Batch 5: loss = 0.000079, learning rate = 0.001000\n",
      "Epoch 1300: Batch 6: loss = 0.000096, learning rate = 0.001000\n",
      "Epoch 1300: Batch 7: loss = 0.000088, learning rate = 0.001000\n",
      "Epoch 1300: Batch 8: loss = 0.000107, learning rate = 0.001000\n",
      "Epoch 1300: Batch 9: loss = 0.000087, learning rate = 0.001000\n",
      "Epoch 1300: Batch 10: loss = 0.000078, learning rate = 0.001000\n",
      "Epoch 1300: Batch 11: loss = 0.000160, learning rate = 0.001000\n",
      "Epoch 1300: Batch 12: loss = 0.000085, learning rate = 0.001000\n",
      "Epoch 1300: Batch 13: loss = 0.000076, learning rate = 0.001000\n",
      "Epoch 1300: Batch 14: loss = 0.000072, learning rate = 0.001000\n",
      "Epoch 1300: Batch 15: loss = 0.000078, learning rate = 0.001000\n",
      "Epoch 1300: Batch 16: loss = 0.000139, learning rate = 0.001000\n",
      "Epoch 1300: Batch 17: loss = 0.000161, learning rate = 0.001000\n",
      "Epoch 1300: Batch 18: loss = 0.000098, learning rate = 0.001000\n",
      "Epoch 1300: Batch 19: loss = 0.000137, learning rate = 0.001000\n",
      "Epoch 1300: Batch 20: loss = 0.000151, learning rate = 0.001000\n",
      "Epoch 1300: Batch 21: loss = 0.000113, learning rate = 0.001000\n",
      "Epoch 1300: Batch 22: loss = 0.000145, learning rate = 0.001000\n",
      "Epoch 1300: Batch 23: loss = 0.000057, learning rate = 0.001000\n",
      "Epoch 1300: Batch 24: loss = 0.000095, learning rate = 0.001000\n",
      "Epoch 1300: Batch 25: loss = 0.000079, learning rate = 0.001000\n",
      "Epoch 1300: Batch 26: loss = 0.000053, learning rate = 0.001000\n",
      "Epoch 1300: Batch 27: loss = 0.000088, learning rate = 0.001000\n",
      "Epoch 1300: Batch 28: loss = 0.000067, learning rate = 0.001000\n",
      "Epoch 1300: Batch 29: loss = 0.000121, learning rate = 0.001000\n",
      "Epoch 1300: Batch 30: loss = 0.000095, learning rate = 0.001000\n",
      "Epoch 1300: Batch 31: loss = 0.000097, learning rate = 0.001000\n",
      "Epoch 1350: Batch 0: loss = 0.000097, learning rate = 0.001000\n",
      "Epoch 1350: Batch 1: loss = 0.000051, learning rate = 0.001000\n",
      "Epoch 1350: Batch 2: loss = 0.000132, learning rate = 0.001000\n",
      "Epoch 1350: Batch 3: loss = 0.000076, learning rate = 0.001000\n",
      "Epoch 1350: Batch 4: loss = 0.000053, learning rate = 0.001000\n",
      "Epoch 1350: Batch 5: loss = 0.000084, learning rate = 0.001000\n",
      "Epoch 1350: Batch 6: loss = 0.000096, learning rate = 0.001000\n",
      "Epoch 1350: Batch 7: loss = 0.000076, learning rate = 0.001000\n",
      "Epoch 1350: Batch 8: loss = 0.000107, learning rate = 0.001000\n",
      "Epoch 1350: Batch 9: loss = 0.000099, learning rate = 0.001000\n",
      "Epoch 1350: Batch 10: loss = 0.000080, learning rate = 0.001000\n",
      "Epoch 1350: Batch 11: loss = 0.000053, learning rate = 0.001000\n",
      "Epoch 1350: Batch 12: loss = 0.000068, learning rate = 0.001000\n",
      "Epoch 1350: Batch 13: loss = 0.000090, learning rate = 0.001000\n",
      "Epoch 1350: Batch 14: loss = 0.000085, learning rate = 0.001000\n",
      "Epoch 1350: Batch 15: loss = 0.000107, learning rate = 0.001000\n",
      "Epoch 1350: Batch 16: loss = 0.000082, learning rate = 0.001000\n",
      "Epoch 1350: Batch 17: loss = 0.000089, learning rate = 0.001000\n",
      "Epoch 1350: Batch 18: loss = 0.000048, learning rate = 0.001000\n",
      "Epoch 1350: Batch 19: loss = 0.000112, learning rate = 0.001000\n",
      "Epoch 1350: Batch 20: loss = 0.000103, learning rate = 0.001000\n",
      "Epoch 1350: Batch 21: loss = 0.000056, learning rate = 0.001000\n",
      "Epoch 1350: Batch 22: loss = 0.000097, learning rate = 0.001000\n",
      "Epoch 1350: Batch 23: loss = 0.000070, learning rate = 0.001000\n",
      "Epoch 1350: Batch 24: loss = 0.000101, learning rate = 0.001000\n",
      "Epoch 1350: Batch 25: loss = 0.000054, learning rate = 0.001000\n",
      "Epoch 1350: Batch 26: loss = 0.000060, learning rate = 0.001000\n",
      "Epoch 1350: Batch 27: loss = 0.000035, learning rate = 0.001000\n",
      "Epoch 1350: Batch 28: loss = 0.000098, learning rate = 0.001000\n",
      "Epoch 1350: Batch 29: loss = 0.000067, learning rate = 0.001000\n",
      "Epoch 1350: Batch 30: loss = 0.000081, learning rate = 0.001000\n",
      "Epoch 1350: Batch 31: loss = 0.000050, learning rate = 0.001000\n",
      "Epoch 1400: Batch 0: loss = 0.000108, learning rate = 0.001000\n",
      "Epoch 1400: Batch 1: loss = 0.000076, learning rate = 0.001000\n",
      "Epoch 1400: Batch 2: loss = 0.000088, learning rate = 0.001000\n",
      "Epoch 1400: Batch 3: loss = 0.000078, learning rate = 0.001000\n",
      "Epoch 1400: Batch 4: loss = 0.000084, learning rate = 0.001000\n",
      "Epoch 1400: Batch 5: loss = 0.000065, learning rate = 0.001000\n",
      "Epoch 1400: Batch 6: loss = 0.000053, learning rate = 0.001000\n",
      "Epoch 1400: Batch 7: loss = 0.000073, learning rate = 0.001000\n",
      "Epoch 1400: Batch 8: loss = 0.000083, learning rate = 0.001000\n",
      "Epoch 1400: Batch 9: loss = 0.000106, learning rate = 0.001000\n",
      "Epoch 1400: Batch 10: loss = 0.000078, learning rate = 0.001000\n",
      "Epoch 1400: Batch 11: loss = 0.000116, learning rate = 0.001000\n",
      "Epoch 1400: Batch 12: loss = 0.000060, learning rate = 0.001000\n",
      "Epoch 1400: Batch 13: loss = 0.000069, learning rate = 0.001000\n",
      "Epoch 1400: Batch 14: loss = 0.000059, learning rate = 0.001000\n",
      "Epoch 1400: Batch 15: loss = 0.000043, learning rate = 0.001000\n",
      "Epoch 1400: Batch 16: loss = 0.000105, learning rate = 0.001000\n",
      "Epoch 1400: Batch 17: loss = 0.000101, learning rate = 0.001000\n",
      "Epoch 1400: Batch 18: loss = 0.000079, learning rate = 0.001000\n",
      "Epoch 1400: Batch 19: loss = 0.000097, learning rate = 0.001000\n",
      "Epoch 1400: Batch 20: loss = 0.000063, learning rate = 0.001000\n",
      "Epoch 1400: Batch 21: loss = 0.000051, learning rate = 0.001000\n",
      "Epoch 1400: Batch 22: loss = 0.000059, learning rate = 0.001000\n",
      "Epoch 1400: Batch 23: loss = 0.000092, learning rate = 0.001000\n",
      "Epoch 1400: Batch 24: loss = 0.000091, learning rate = 0.001000\n",
      "Epoch 1400: Batch 25: loss = 0.000065, learning rate = 0.001000\n",
      "Epoch 1400: Batch 26: loss = 0.000084, learning rate = 0.001000\n",
      "Epoch 1400: Batch 27: loss = 0.000057, learning rate = 0.001000\n",
      "Epoch 1400: Batch 28: loss = 0.000135, learning rate = 0.001000\n",
      "Epoch 1400: Batch 29: loss = 0.000125, learning rate = 0.001000\n",
      "Epoch 1400: Batch 30: loss = 0.000105, learning rate = 0.001000\n",
      "Epoch 1400: Batch 31: loss = 0.000060, learning rate = 0.001000\n",
      "Epoch 1450: Batch 0: loss = 0.000089, learning rate = 0.001000\n",
      "Epoch 1450: Batch 1: loss = 0.000079, learning rate = 0.001000\n",
      "Epoch 1450: Batch 2: loss = 0.000133, learning rate = 0.001000\n",
      "Epoch 1450: Batch 3: loss = 0.000060, learning rate = 0.001000\n",
      "Epoch 1450: Batch 4: loss = 0.000094, learning rate = 0.001000\n",
      "Epoch 1450: Batch 5: loss = 0.000128, learning rate = 0.001000\n",
      "Epoch 1450: Batch 6: loss = 0.000060, learning rate = 0.001000\n",
      "Epoch 1450: Batch 7: loss = 0.000092, learning rate = 0.001000\n",
      "Epoch 1450: Batch 8: loss = 0.000052, learning rate = 0.001000\n",
      "Epoch 1450: Batch 9: loss = 0.000105, learning rate = 0.001000\n",
      "Epoch 1450: Batch 10: loss = 0.000085, learning rate = 0.001000\n",
      "Epoch 1450: Batch 11: loss = 0.000050, learning rate = 0.001000\n",
      "Epoch 1450: Batch 12: loss = 0.000084, learning rate = 0.001000\n",
      "Epoch 1450: Batch 13: loss = 0.000106, learning rate = 0.001000\n",
      "Epoch 1450: Batch 14: loss = 0.000099, learning rate = 0.001000\n",
      "Epoch 1450: Batch 15: loss = 0.000052, learning rate = 0.001000\n",
      "Epoch 1450: Batch 16: loss = 0.000105, learning rate = 0.001000\n",
      "Epoch 1450: Batch 17: loss = 0.000057, learning rate = 0.001000\n",
      "Epoch 1450: Batch 18: loss = 0.000097, learning rate = 0.001000\n",
      "Epoch 1450: Batch 19: loss = 0.000081, learning rate = 0.001000\n",
      "Epoch 1450: Batch 20: loss = 0.000128, learning rate = 0.001000\n",
      "Epoch 1450: Batch 21: loss = 0.000105, learning rate = 0.001000\n",
      "Epoch 1450: Batch 22: loss = 0.000075, learning rate = 0.001000\n",
      "Epoch 1450: Batch 23: loss = 0.000105, learning rate = 0.001000\n",
      "Epoch 1450: Batch 24: loss = 0.000061, learning rate = 0.001000\n",
      "Epoch 1450: Batch 25: loss = 0.000054, learning rate = 0.001000\n",
      "Epoch 1450: Batch 26: loss = 0.000056, learning rate = 0.001000\n",
      "Epoch 1450: Batch 27: loss = 0.000043, learning rate = 0.001000\n",
      "Epoch 1450: Batch 28: loss = 0.000134, learning rate = 0.001000\n",
      "Epoch 1450: Batch 29: loss = 0.000063, learning rate = 0.001000\n",
      "Epoch 1450: Batch 30: loss = 0.000062, learning rate = 0.001000\n",
      "Epoch 1450: Batch 31: loss = 0.000101, learning rate = 0.001000\n",
      "Epoch 1500: Batch 0: loss = 0.000096, learning rate = 0.001000\n",
      "Epoch 1500: Batch 1: loss = 0.000117, learning rate = 0.001000\n",
      "Epoch 1500: Batch 2: loss = 0.000065, learning rate = 0.001000\n",
      "Epoch 1500: Batch 3: loss = 0.000069, learning rate = 0.001000\n",
      "Epoch 1500: Batch 4: loss = 0.000055, learning rate = 0.001000\n",
      "Epoch 1500: Batch 5: loss = 0.000105, learning rate = 0.001000\n",
      "Epoch 1500: Batch 6: loss = 0.000114, learning rate = 0.001000\n",
      "Epoch 1500: Batch 7: loss = 0.000123, learning rate = 0.001000\n",
      "Epoch 1500: Batch 8: loss = 0.000063, learning rate = 0.001000\n",
      "Epoch 1500: Batch 9: loss = 0.000106, learning rate = 0.001000\n",
      "Epoch 1500: Batch 10: loss = 0.000056, learning rate = 0.001000\n",
      "Epoch 1500: Batch 11: loss = 0.000041, learning rate = 0.001000\n",
      "Epoch 1500: Batch 12: loss = 0.000066, learning rate = 0.001000\n",
      "Epoch 1500: Batch 13: loss = 0.000089, learning rate = 0.001000\n",
      "Epoch 1500: Batch 14: loss = 0.000090, learning rate = 0.001000\n",
      "Epoch 1500: Batch 15: loss = 0.000104, learning rate = 0.001000\n",
      "Epoch 1500: Batch 16: loss = 0.000049, learning rate = 0.001000\n",
      "Epoch 1500: Batch 17: loss = 0.000059, learning rate = 0.001000\n",
      "Epoch 1500: Batch 18: loss = 0.000086, learning rate = 0.001000\n",
      "Epoch 1500: Batch 19: loss = 0.000088, learning rate = 0.001000\n",
      "Epoch 1500: Batch 20: loss = 0.000094, learning rate = 0.001000\n",
      "Epoch 1500: Batch 21: loss = 0.000069, learning rate = 0.001000\n",
      "Epoch 1500: Batch 22: loss = 0.000074, learning rate = 0.001000\n",
      "Epoch 1500: Batch 23: loss = 0.000043, learning rate = 0.001000\n",
      "Epoch 1500: Batch 24: loss = 0.000081, learning rate = 0.001000\n",
      "Epoch 1500: Batch 25: loss = 0.000054, learning rate = 0.001000\n",
      "Epoch 1500: Batch 26: loss = 0.000088, learning rate = 0.001000\n",
      "Epoch 1500: Batch 27: loss = 0.000073, learning rate = 0.001000\n",
      "Epoch 1500: Batch 28: loss = 0.000066, learning rate = 0.001000\n",
      "Epoch 1500: Batch 29: loss = 0.000088, learning rate = 0.001000\n",
      "Epoch 1500: Batch 30: loss = 0.000055, learning rate = 0.001000\n",
      "Epoch 1500: Batch 31: loss = 0.000024, learning rate = 0.001000\n",
      "Epoch 1550: Batch 0: loss = 0.000081, learning rate = 0.001000\n",
      "Epoch 1550: Batch 1: loss = 0.000092, learning rate = 0.001000\n",
      "Epoch 1550: Batch 2: loss = 0.000060, learning rate = 0.001000\n",
      "Epoch 1550: Batch 3: loss = 0.000081, learning rate = 0.001000\n",
      "Epoch 1550: Batch 4: loss = 0.000060, learning rate = 0.001000\n",
      "Epoch 1550: Batch 5: loss = 0.000055, learning rate = 0.001000\n",
      "Epoch 1550: Batch 6: loss = 0.000059, learning rate = 0.001000\n",
      "Epoch 1550: Batch 7: loss = 0.000088, learning rate = 0.001000\n",
      "Epoch 1550: Batch 8: loss = 0.000065, learning rate = 0.001000\n",
      "Epoch 1550: Batch 9: loss = 0.000057, learning rate = 0.001000\n",
      "Epoch 1550: Batch 10: loss = 0.000112, learning rate = 0.001000\n",
      "Epoch 1550: Batch 11: loss = 0.000071, learning rate = 0.001000\n",
      "Epoch 1550: Batch 12: loss = 0.000077, learning rate = 0.001000\n",
      "Epoch 1550: Batch 13: loss = 0.000114, learning rate = 0.001000\n",
      "Epoch 1550: Batch 14: loss = 0.000114, learning rate = 0.001000\n",
      "Epoch 1550: Batch 15: loss = 0.000092, learning rate = 0.001000\n",
      "Epoch 1550: Batch 16: loss = 0.000096, learning rate = 0.001000\n",
      "Epoch 1550: Batch 17: loss = 0.000068, learning rate = 0.001000\n",
      "Epoch 1550: Batch 18: loss = 0.000083, learning rate = 0.001000\n",
      "Epoch 1550: Batch 19: loss = 0.000119, learning rate = 0.001000\n",
      "Epoch 1550: Batch 20: loss = 0.000093, learning rate = 0.001000\n",
      "Epoch 1550: Batch 21: loss = 0.000042, learning rate = 0.001000\n",
      "Epoch 1550: Batch 22: loss = 0.000059, learning rate = 0.001000\n",
      "Epoch 1550: Batch 23: loss = 0.000077, learning rate = 0.001000\n",
      "Epoch 1550: Batch 24: loss = 0.000119, learning rate = 0.001000\n",
      "Epoch 1550: Batch 25: loss = 0.000065, learning rate = 0.001000\n",
      "Epoch 1550: Batch 26: loss = 0.000098, learning rate = 0.001000\n",
      "Epoch 1550: Batch 27: loss = 0.000072, learning rate = 0.001000\n",
      "Epoch 1550: Batch 28: loss = 0.000109, learning rate = 0.001000\n",
      "Epoch 1550: Batch 29: loss = 0.000053, learning rate = 0.001000\n",
      "Epoch 1550: Batch 30: loss = 0.000064, learning rate = 0.001000\n",
      "Epoch 1550: Batch 31: loss = 0.000047, learning rate = 0.001000\n",
      "Epoch 1600: Batch 0: loss = 0.000102, learning rate = 0.001000\n",
      "Epoch 1600: Batch 1: loss = 0.000070, learning rate = 0.001000\n",
      "Epoch 1600: Batch 2: loss = 0.000116, learning rate = 0.001000\n",
      "Epoch 1600: Batch 3: loss = 0.000055, learning rate = 0.001000\n",
      "Epoch 1600: Batch 4: loss = 0.000067, learning rate = 0.001000\n",
      "Epoch 1600: Batch 5: loss = 0.000074, learning rate = 0.001000\n",
      "Epoch 1600: Batch 6: loss = 0.000084, learning rate = 0.001000\n",
      "Epoch 1600: Batch 7: loss = 0.000124, learning rate = 0.001000\n",
      "Epoch 1600: Batch 8: loss = 0.000092, learning rate = 0.001000\n",
      "Epoch 1600: Batch 9: loss = 0.000079, learning rate = 0.001000\n",
      "Epoch 1600: Batch 10: loss = 0.000071, learning rate = 0.001000\n",
      "Epoch 1600: Batch 11: loss = 0.000112, learning rate = 0.001000\n",
      "Epoch 1600: Batch 12: loss = 0.000084, learning rate = 0.001000\n",
      "Epoch 1600: Batch 13: loss = 0.000070, learning rate = 0.001000\n",
      "Epoch 1600: Batch 14: loss = 0.000061, learning rate = 0.001000\n",
      "Epoch 1600: Batch 15: loss = 0.000101, learning rate = 0.001000\n",
      "Epoch 1600: Batch 16: loss = 0.000069, learning rate = 0.001000\n",
      "Epoch 1600: Batch 17: loss = 0.000061, learning rate = 0.001000\n",
      "Epoch 1600: Batch 18: loss = 0.000044, learning rate = 0.001000\n",
      "Epoch 1600: Batch 19: loss = 0.000063, learning rate = 0.001000\n",
      "Epoch 1600: Batch 20: loss = 0.000055, learning rate = 0.001000\n",
      "Epoch 1600: Batch 21: loss = 0.000056, learning rate = 0.001000\n",
      "Epoch 1600: Batch 22: loss = 0.000061, learning rate = 0.001000\n",
      "Epoch 1600: Batch 23: loss = 0.000077, learning rate = 0.001000\n",
      "Epoch 1600: Batch 24: loss = 0.000051, learning rate = 0.001000\n",
      "Epoch 1600: Batch 25: loss = 0.000057, learning rate = 0.001000\n",
      "Epoch 1600: Batch 26: loss = 0.000050, learning rate = 0.001000\n",
      "Epoch 1600: Batch 27: loss = 0.000106, learning rate = 0.001000\n",
      "Epoch 1600: Batch 28: loss = 0.000090, learning rate = 0.001000\n",
      "Epoch 1600: Batch 29: loss = 0.000090, learning rate = 0.001000\n",
      "Epoch 1600: Batch 30: loss = 0.000089, learning rate = 0.001000\n",
      "Epoch 1600: Batch 31: loss = 0.000102, learning rate = 0.001000\n",
      "Epoch 1650: Batch 0: loss = 0.000056, learning rate = 0.001000\n",
      "Epoch 1650: Batch 1: loss = 0.000091, learning rate = 0.001000\n",
      "Epoch 1650: Batch 2: loss = 0.000050, learning rate = 0.001000\n",
      "Epoch 1650: Batch 3: loss = 0.000062, learning rate = 0.001000\n",
      "Epoch 1650: Batch 4: loss = 0.000076, learning rate = 0.001000\n",
      "Epoch 1650: Batch 5: loss = 0.000079, learning rate = 0.001000\n",
      "Epoch 1650: Batch 6: loss = 0.000118, learning rate = 0.001000\n",
      "Epoch 1650: Batch 7: loss = 0.000045, learning rate = 0.001000\n",
      "Epoch 1650: Batch 8: loss = 0.000074, learning rate = 0.001000\n",
      "Epoch 1650: Batch 9: loss = 0.000115, learning rate = 0.001000\n",
      "Epoch 1650: Batch 10: loss = 0.000073, learning rate = 0.001000\n",
      "Epoch 1650: Batch 11: loss = 0.000068, learning rate = 0.001000\n",
      "Epoch 1650: Batch 12: loss = 0.000058, learning rate = 0.001000\n",
      "Epoch 1650: Batch 13: loss = 0.000092, learning rate = 0.001000\n",
      "Epoch 1650: Batch 14: loss = 0.000088, learning rate = 0.001000\n",
      "Epoch 1650: Batch 15: loss = 0.000118, learning rate = 0.001000\n",
      "Epoch 1650: Batch 16: loss = 0.000071, learning rate = 0.001000\n",
      "Epoch 1650: Batch 17: loss = 0.000112, learning rate = 0.001000\n",
      "Epoch 1650: Batch 18: loss = 0.000119, learning rate = 0.001000\n",
      "Epoch 1650: Batch 19: loss = 0.000079, learning rate = 0.001000\n",
      "Epoch 1650: Batch 20: loss = 0.000106, learning rate = 0.001000\n",
      "Epoch 1650: Batch 21: loss = 0.000090, learning rate = 0.001000\n",
      "Epoch 1650: Batch 22: loss = 0.000092, learning rate = 0.001000\n",
      "Epoch 1650: Batch 23: loss = 0.000054, learning rate = 0.001000\n",
      "Epoch 1650: Batch 24: loss = 0.000081, learning rate = 0.001000\n",
      "Epoch 1650: Batch 25: loss = 0.000073, learning rate = 0.001000\n",
      "Epoch 1650: Batch 26: loss = 0.000108, learning rate = 0.001000\n",
      "Epoch 1650: Batch 27: loss = 0.000099, learning rate = 0.001000\n",
      "Epoch 1650: Batch 28: loss = 0.000054, learning rate = 0.001000\n",
      "Epoch 1650: Batch 29: loss = 0.000113, learning rate = 0.001000\n",
      "Epoch 1650: Batch 30: loss = 0.000043, learning rate = 0.001000\n",
      "Epoch 1650: Batch 31: loss = 0.000082, learning rate = 0.001000\n",
      "Epoch 1700: Batch 0: loss = 0.000060, learning rate = 0.001000\n",
      "Epoch 1700: Batch 1: loss = 0.000067, learning rate = 0.001000\n",
      "Epoch 1700: Batch 2: loss = 0.000057, learning rate = 0.001000\n",
      "Epoch 1700: Batch 3: loss = 0.000081, learning rate = 0.001000\n",
      "Epoch 1700: Batch 4: loss = 0.000082, learning rate = 0.001000\n",
      "Epoch 1700: Batch 5: loss = 0.000064, learning rate = 0.001000\n",
      "Epoch 1700: Batch 6: loss = 0.000081, learning rate = 0.001000\n",
      "Epoch 1700: Batch 7: loss = 0.000074, learning rate = 0.001000\n",
      "Epoch 1700: Batch 8: loss = 0.000062, learning rate = 0.001000\n",
      "Epoch 1700: Batch 9: loss = 0.000085, learning rate = 0.001000\n",
      "Epoch 1700: Batch 10: loss = 0.000058, learning rate = 0.001000\n",
      "Epoch 1700: Batch 11: loss = 0.000058, learning rate = 0.001000\n",
      "Epoch 1700: Batch 12: loss = 0.000089, learning rate = 0.001000\n",
      "Epoch 1700: Batch 13: loss = 0.000070, learning rate = 0.001000\n",
      "Epoch 1700: Batch 14: loss = 0.000067, learning rate = 0.001000\n",
      "Epoch 1700: Batch 15: loss = 0.000082, learning rate = 0.001000\n",
      "Epoch 1700: Batch 16: loss = 0.000060, learning rate = 0.001000\n",
      "Epoch 1700: Batch 17: loss = 0.000037, learning rate = 0.001000\n",
      "Epoch 1700: Batch 18: loss = 0.000056, learning rate = 0.001000\n",
      "Epoch 1700: Batch 19: loss = 0.000036, learning rate = 0.001000\n",
      "Epoch 1700: Batch 20: loss = 0.000058, learning rate = 0.001000\n",
      "Epoch 1700: Batch 21: loss = 0.000105, learning rate = 0.001000\n",
      "Epoch 1700: Batch 22: loss = 0.000095, learning rate = 0.001000\n",
      "Epoch 1700: Batch 23: loss = 0.000061, learning rate = 0.001000\n",
      "Epoch 1700: Batch 24: loss = 0.000062, learning rate = 0.001000\n",
      "Epoch 1700: Batch 25: loss = 0.000146, learning rate = 0.001000\n",
      "Epoch 1700: Batch 26: loss = 0.000069, learning rate = 0.001000\n",
      "Epoch 1700: Batch 27: loss = 0.000122, learning rate = 0.001000\n",
      "Epoch 1700: Batch 28: loss = 0.000177, learning rate = 0.001000\n",
      "Epoch 1700: Batch 29: loss = 0.000056, learning rate = 0.001000\n",
      "Epoch 1700: Batch 30: loss = 0.000091, learning rate = 0.001000\n",
      "Epoch 1700: Batch 31: loss = 0.000048, learning rate = 0.001000\n",
      "Epoch 1750: Batch 0: loss = 0.000076, learning rate = 0.001000\n",
      "Epoch 1750: Batch 1: loss = 0.000073, learning rate = 0.001000\n",
      "Epoch 1750: Batch 2: loss = 0.000039, learning rate = 0.001000\n",
      "Epoch 1750: Batch 3: loss = 0.000035, learning rate = 0.001000\n",
      "Epoch 1750: Batch 4: loss = 0.000069, learning rate = 0.001000\n",
      "Epoch 1750: Batch 5: loss = 0.000053, learning rate = 0.001000\n",
      "Epoch 1750: Batch 6: loss = 0.000048, learning rate = 0.001000\n",
      "Epoch 1750: Batch 7: loss = 0.000050, learning rate = 0.001000\n",
      "Epoch 1750: Batch 8: loss = 0.000037, learning rate = 0.001000\n",
      "Epoch 1750: Batch 9: loss = 0.000131, learning rate = 0.001000\n",
      "Epoch 1750: Batch 10: loss = 0.000074, learning rate = 0.001000\n",
      "Epoch 1750: Batch 11: loss = 0.000063, learning rate = 0.001000\n",
      "Epoch 1750: Batch 12: loss = 0.000056, learning rate = 0.001000\n",
      "Epoch 1750: Batch 13: loss = 0.000065, learning rate = 0.001000\n",
      "Epoch 1750: Batch 14: loss = 0.000062, learning rate = 0.001000\n",
      "Epoch 1750: Batch 15: loss = 0.000066, learning rate = 0.001000\n",
      "Epoch 1750: Batch 16: loss = 0.000052, learning rate = 0.001000\n",
      "Epoch 1750: Batch 17: loss = 0.000072, learning rate = 0.001000\n",
      "Epoch 1750: Batch 18: loss = 0.000072, learning rate = 0.001000\n",
      "Epoch 1750: Batch 19: loss = 0.000097, learning rate = 0.001000\n",
      "Epoch 1750: Batch 20: loss = 0.000120, learning rate = 0.001000\n",
      "Epoch 1750: Batch 21: loss = 0.000085, learning rate = 0.001000\n",
      "Epoch 1750: Batch 22: loss = 0.000100, learning rate = 0.001000\n",
      "Epoch 1750: Batch 23: loss = 0.000135, learning rate = 0.001000\n",
      "Epoch 1750: Batch 24: loss = 0.000088, learning rate = 0.001000\n",
      "Epoch 1750: Batch 25: loss = 0.000098, learning rate = 0.001000\n",
      "Epoch 1750: Batch 26: loss = 0.000086, learning rate = 0.001000\n",
      "Epoch 1750: Batch 27: loss = 0.000078, learning rate = 0.001000\n",
      "Epoch 1750: Batch 28: loss = 0.000057, learning rate = 0.001000\n",
      "Epoch 1750: Batch 29: loss = 0.000044, learning rate = 0.001000\n",
      "Epoch 1750: Batch 30: loss = 0.000076, learning rate = 0.001000\n",
      "Epoch 1750: Batch 31: loss = 0.000020, learning rate = 0.001000\n",
      "Epoch 1800: Batch 0: loss = 0.000054, learning rate = 0.001000\n",
      "Epoch 1800: Batch 1: loss = 0.000035, learning rate = 0.001000\n",
      "Epoch 1800: Batch 2: loss = 0.000129, learning rate = 0.001000\n",
      "Epoch 1800: Batch 3: loss = 0.000063, learning rate = 0.001000\n",
      "Epoch 1800: Batch 4: loss = 0.000085, learning rate = 0.001000\n",
      "Epoch 1800: Batch 5: loss = 0.000057, learning rate = 0.001000\n",
      "Epoch 1800: Batch 6: loss = 0.000052, learning rate = 0.001000\n",
      "Epoch 1800: Batch 7: loss = 0.000040, learning rate = 0.001000\n",
      "Epoch 1800: Batch 8: loss = 0.000063, learning rate = 0.001000\n",
      "Epoch 1800: Batch 9: loss = 0.000057, learning rate = 0.001000\n",
      "Epoch 1800: Batch 10: loss = 0.000070, learning rate = 0.001000\n",
      "Epoch 1800: Batch 11: loss = 0.000069, learning rate = 0.001000\n",
      "Epoch 1800: Batch 12: loss = 0.000065, learning rate = 0.001000\n",
      "Epoch 1800: Batch 13: loss = 0.000092, learning rate = 0.001000\n",
      "Epoch 1800: Batch 14: loss = 0.000098, learning rate = 0.001000\n",
      "Epoch 1800: Batch 15: loss = 0.000098, learning rate = 0.001000\n",
      "Epoch 1800: Batch 16: loss = 0.000131, learning rate = 0.001000\n",
      "Epoch 1800: Batch 17: loss = 0.000048, learning rate = 0.001000\n",
      "Epoch 1800: Batch 18: loss = 0.000076, learning rate = 0.001000\n",
      "Epoch 1800: Batch 19: loss = 0.000058, learning rate = 0.001000\n",
      "Epoch 1800: Batch 20: loss = 0.000086, learning rate = 0.001000\n",
      "Epoch 1800: Batch 21: loss = 0.000083, learning rate = 0.001000\n",
      "Epoch 1800: Batch 22: loss = 0.000053, learning rate = 0.001000\n",
      "Epoch 1800: Batch 23: loss = 0.000082, learning rate = 0.001000\n",
      "Epoch 1800: Batch 24: loss = 0.000054, learning rate = 0.001000\n",
      "Epoch 1800: Batch 25: loss = 0.000068, learning rate = 0.001000\n",
      "Epoch 1800: Batch 26: loss = 0.000090, learning rate = 0.001000\n",
      "Epoch 1800: Batch 27: loss = 0.000075, learning rate = 0.001000\n",
      "Epoch 1800: Batch 28: loss = 0.000068, learning rate = 0.001000\n",
      "Epoch 1800: Batch 29: loss = 0.000075, learning rate = 0.001000\n",
      "Epoch 1800: Batch 30: loss = 0.000058, learning rate = 0.001000\n",
      "Epoch 1800: Batch 31: loss = 0.000050, learning rate = 0.001000\n",
      "Epoch 1850: Batch 0: loss = 0.000040, learning rate = 0.001000\n",
      "Epoch 1850: Batch 1: loss = 0.000121, learning rate = 0.001000\n",
      "Epoch 1850: Batch 2: loss = 0.000069, learning rate = 0.001000\n",
      "Epoch 1850: Batch 3: loss = 0.000061, learning rate = 0.001000\n",
      "Epoch 1850: Batch 4: loss = 0.000060, learning rate = 0.001000\n",
      "Epoch 1850: Batch 5: loss = 0.000089, learning rate = 0.001000\n",
      "Epoch 1850: Batch 6: loss = 0.000046, learning rate = 0.001000\n",
      "Epoch 1850: Batch 7: loss = 0.000077, learning rate = 0.001000\n",
      "Epoch 1850: Batch 8: loss = 0.000070, learning rate = 0.001000\n",
      "Epoch 1850: Batch 9: loss = 0.000053, learning rate = 0.001000\n",
      "Epoch 1850: Batch 10: loss = 0.000062, learning rate = 0.001000\n",
      "Epoch 1850: Batch 11: loss = 0.000062, learning rate = 0.001000\n",
      "Epoch 1850: Batch 12: loss = 0.000045, learning rate = 0.001000\n",
      "Epoch 1850: Batch 13: loss = 0.000145, learning rate = 0.001000\n",
      "Epoch 1850: Batch 14: loss = 0.000044, learning rate = 0.001000\n",
      "Epoch 1850: Batch 15: loss = 0.000040, learning rate = 0.001000\n",
      "Epoch 1850: Batch 16: loss = 0.000069, learning rate = 0.001000\n",
      "Epoch 1850: Batch 17: loss = 0.000086, learning rate = 0.001000\n",
      "Epoch 1850: Batch 18: loss = 0.000098, learning rate = 0.001000\n",
      "Epoch 1850: Batch 19: loss = 0.000060, learning rate = 0.001000\n",
      "Epoch 1850: Batch 20: loss = 0.000065, learning rate = 0.001000\n",
      "Epoch 1850: Batch 21: loss = 0.000088, learning rate = 0.001000\n",
      "Epoch 1850: Batch 22: loss = 0.000074, learning rate = 0.001000\n",
      "Epoch 1850: Batch 23: loss = 0.000078, learning rate = 0.001000\n",
      "Epoch 1850: Batch 24: loss = 0.000101, learning rate = 0.001000\n",
      "Epoch 1850: Batch 25: loss = 0.000032, learning rate = 0.001000\n",
      "Epoch 1850: Batch 26: loss = 0.000082, learning rate = 0.001000\n",
      "Epoch 1850: Batch 27: loss = 0.000063, learning rate = 0.001000\n",
      "Epoch 1850: Batch 28: loss = 0.000090, learning rate = 0.001000\n",
      "Epoch 1850: Batch 29: loss = 0.000044, learning rate = 0.001000\n",
      "Epoch 1850: Batch 30: loss = 0.000070, learning rate = 0.001000\n",
      "Epoch 1850: Batch 31: loss = 0.000066, learning rate = 0.001000\n",
      "Epoch 1900: Batch 0: loss = 0.000049, learning rate = 0.001000\n",
      "Epoch 1900: Batch 1: loss = 0.000068, learning rate = 0.001000\n",
      "Epoch 1900: Batch 2: loss = 0.000102, learning rate = 0.001000\n",
      "Epoch 1900: Batch 3: loss = 0.000062, learning rate = 0.001000\n",
      "Epoch 1900: Batch 4: loss = 0.000101, learning rate = 0.001000\n",
      "Epoch 1900: Batch 5: loss = 0.000068, learning rate = 0.001000\n",
      "Epoch 1900: Batch 6: loss = 0.000054, learning rate = 0.001000\n",
      "Epoch 1900: Batch 7: loss = 0.000087, learning rate = 0.001000\n",
      "Epoch 1900: Batch 8: loss = 0.000052, learning rate = 0.001000\n",
      "Epoch 1900: Batch 9: loss = 0.000062, learning rate = 0.001000\n",
      "Epoch 1900: Batch 10: loss = 0.000032, learning rate = 0.001000\n",
      "Epoch 1900: Batch 11: loss = 0.000078, learning rate = 0.001000\n",
      "Epoch 1900: Batch 12: loss = 0.000104, learning rate = 0.001000\n",
      "Epoch 1900: Batch 13: loss = 0.000065, learning rate = 0.001000\n",
      "Epoch 1900: Batch 14: loss = 0.000067, learning rate = 0.001000\n",
      "Epoch 1900: Batch 15: loss = 0.000057, learning rate = 0.001000\n",
      "Epoch 1900: Batch 16: loss = 0.000124, learning rate = 0.001000\n",
      "Epoch 1900: Batch 17: loss = 0.000050, learning rate = 0.001000\n",
      "Epoch 1900: Batch 18: loss = 0.000100, learning rate = 0.001000\n",
      "Epoch 1900: Batch 19: loss = 0.000099, learning rate = 0.001000\n",
      "Epoch 1900: Batch 20: loss = 0.000077, learning rate = 0.001000\n",
      "Epoch 1900: Batch 21: loss = 0.000099, learning rate = 0.001000\n",
      "Epoch 1900: Batch 22: loss = 0.000055, learning rate = 0.001000\n",
      "Epoch 1900: Batch 23: loss = 0.000074, learning rate = 0.001000\n",
      "Epoch 1900: Batch 24: loss = 0.000091, learning rate = 0.001000\n",
      "Epoch 1900: Batch 25: loss = 0.000087, learning rate = 0.001000\n",
      "Epoch 1900: Batch 26: loss = 0.000033, learning rate = 0.001000\n",
      "Epoch 1900: Batch 27: loss = 0.000045, learning rate = 0.001000\n",
      "Epoch 1900: Batch 28: loss = 0.000039, learning rate = 0.001000\n",
      "Epoch 1900: Batch 29: loss = 0.000066, learning rate = 0.001000\n",
      "Epoch 1900: Batch 30: loss = 0.000054, learning rate = 0.001000\n",
      "Epoch 1900: Batch 31: loss = 0.000048, learning rate = 0.001000\n",
      "Epoch 1950: Batch 0: loss = 0.000099, learning rate = 0.001000\n",
      "Epoch 1950: Batch 1: loss = 0.000049, learning rate = 0.001000\n",
      "Epoch 1950: Batch 2: loss = 0.000087, learning rate = 0.001000\n",
      "Epoch 1950: Batch 3: loss = 0.000108, learning rate = 0.001000\n",
      "Epoch 1950: Batch 4: loss = 0.000039, learning rate = 0.001000\n",
      "Epoch 1950: Batch 5: loss = 0.000064, learning rate = 0.001000\n",
      "Epoch 1950: Batch 6: loss = 0.000045, learning rate = 0.001000\n",
      "Epoch 1950: Batch 7: loss = 0.000077, learning rate = 0.001000\n",
      "Epoch 1950: Batch 8: loss = 0.000046, learning rate = 0.001000\n",
      "Epoch 1950: Batch 9: loss = 0.000069, learning rate = 0.001000\n",
      "Epoch 1950: Batch 10: loss = 0.000076, learning rate = 0.001000\n",
      "Epoch 1950: Batch 11: loss = 0.000093, learning rate = 0.001000\n",
      "Epoch 1950: Batch 12: loss = 0.000105, learning rate = 0.001000\n",
      "Epoch 1950: Batch 13: loss = 0.000045, learning rate = 0.001000\n",
      "Epoch 1950: Batch 14: loss = 0.000082, learning rate = 0.001000\n",
      "Epoch 1950: Batch 15: loss = 0.000107, learning rate = 0.001000\n",
      "Epoch 1950: Batch 16: loss = 0.000104, learning rate = 0.001000\n",
      "Epoch 1950: Batch 17: loss = 0.000065, learning rate = 0.001000\n",
      "Epoch 1950: Batch 18: loss = 0.000132, learning rate = 0.001000\n",
      "Epoch 1950: Batch 19: loss = 0.000062, learning rate = 0.001000\n",
      "Epoch 1950: Batch 20: loss = 0.000062, learning rate = 0.001000\n",
      "Epoch 1950: Batch 21: loss = 0.000076, learning rate = 0.001000\n",
      "Epoch 1950: Batch 22: loss = 0.000040, learning rate = 0.001000\n",
      "Epoch 1950: Batch 23: loss = 0.000083, learning rate = 0.001000\n",
      "Epoch 1950: Batch 24: loss = 0.000051, learning rate = 0.001000\n",
      "Epoch 1950: Batch 25: loss = 0.000060, learning rate = 0.001000\n",
      "Epoch 1950: Batch 26: loss = 0.000071, learning rate = 0.001000\n",
      "Epoch 1950: Batch 27: loss = 0.000102, learning rate = 0.001000\n",
      "Epoch 1950: Batch 28: loss = 0.000089, learning rate = 0.001000\n",
      "Epoch 1950: Batch 29: loss = 0.000105, learning rate = 0.001000\n",
      "Epoch 1950: Batch 30: loss = 0.000068, learning rate = 0.001000\n",
      "Epoch 1950: Batch 31: loss = 0.000124, learning rate = 0.001000\n"
     ]
    }
   ],
   "source": [
    "print('Shape of train data')\n",
    "print(inputs_train.shape, outputs_train.shape)\n",
    "print('#'*100)\n",
    "\n",
    "bs = 64 # Batch size\n",
    "# Calculate the number of batches\n",
    "num_batches = len(inputs_train) // bs\n",
    "# print(\"Number of batches:\", num_batches)\n",
    "        \n",
    "# Training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=16000, gamma=1.0) # gamma=0.8\n",
    "\n",
    "iteration_list, loss_list, learningrates_list = [], [], []\n",
    "iteration = 0\n",
    "\n",
    "n_epochs = 2000 #100 # 10  #800\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    # Shuffle the train data using the generated indices\n",
    "    num_samples = len(inputs_train)\n",
    "    indices = torch.randperm(num_samples).to(device) # Generate random permutation of indices\n",
    "    inputs_train_shuffled = inputs_train[indices]\n",
    "    outputs_train_shuffled = outputs_train[indices]\n",
    "    \n",
    "    # Initialize lists to store batches\n",
    "    inputs_train_batches = []\n",
    "    outputs_train_batches = []\n",
    "    # Split the data into batches\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * bs\n",
    "        end_idx = (i + 1) * bs\n",
    "        inputs_train_batches.append(inputs_train_shuffled[start_idx:end_idx])\n",
    "        outputs_train_batches.append(outputs_train_shuffled[start_idx:end_idx])\n",
    "    # Handle leftover data into the last batch\n",
    "    if len(inputs_train_shuffled) % bs != 0:\n",
    "        start_idx = num_batches * bs\n",
    "        inputs_train_batches.append(inputs_train_shuffled[start_idx:])\n",
    "        outputs_train_batches.append(outputs_train_shuffled[start_idx:])\n",
    "    \n",
    "    for i, (inputs_batch, outputs_batch) in enumerate(zip(inputs_train_batches, outputs_train_batches)):\n",
    "        #print(f\"Shape of inputs_train_batch[{i}]:\", inputs_batch.shape) # (bs, nx)\n",
    "        #print(f\"Shape of outputs_train_batch[{i}]:\", outputs_batch.shape) # (bs, nt, nx)\n",
    "        \n",
    "        branch_inputs = inputs_batch # (bs, m) = (bs, nx) \n",
    "        \n",
    "        trunk_inputs = grid # (neval, 2) = (nt*nx, 2)\n",
    "            \n",
    "        outputs_needed = outputs_batch.reshape(-1, nt*nx) # (bs, neval) = (bs, nt*nx)\n",
    "\n",
    "        # print(branch_inputs.shape, trunk_inputs.shape, outputs_needed.shape)   \n",
    "        # print('*********')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predicted_values = model(branch_inputs, trunk_inputs) # (bs, nt*nx)\n",
    "        target_values = outputs_needed # (bs, nt*nx)\n",
    "        loss = nn.MSELoss()(predicted_values, target_values)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            print('Epoch %s:' % epoch, 'Batch %s:' % i, 'loss = %f,' % loss,\n",
    "                  'learning rate = %f' % optimizer.state_dict()['param_groups'][0]['lr']) \n",
    "        \n",
    "        iteration_list.append(iteration)\n",
    "        loss_list.append(loss.item())\n",
    "        learningrates_list.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "        iteration+=1\n",
    "    \n",
    "if save == True:\n",
    "    np.save(os.path.join(resultdir, f'iteration_list_{modeltype}.npy'), np.asarray(iteration_list))\n",
    "    np.save(os.path.join(resultdir, f'loss_list_{modeltype}.npy'), np.asarray(loss_list))\n",
    "    np.save(os.path.join(resultdir, f'learningrates_list_{modeltype}.npy'), np.asarray(learningrates_list))\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(iteration_list, loss_list, 'g', label = 'training loss')\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Training loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "if save == True:\n",
    "    plt.savefig(os.path.join(resultdir, f'loss_plot_{modeltype}.jpg'))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(iteration_list, learningrates_list, 'b', label = 'learning-rate')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Learning-rate')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "if save == True:\n",
    "    plt.savefig(os.path.join(resultdir, f'learning-rate_plot_{modeltype}.jpg'))\n",
    "    \n",
    "# end timer\n",
    "finish = time.time() - start  # time for network to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a67807e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save == True:\n",
    "    torch.save(model.state_dict(), os.path.join(resultdir, f'model_state_dict_{modeltype}.pt'))\n",
    "# model.load_state_dict(torch.load(os.path.join(resultdir,'model_state_dict.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47234a6a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST SAMPLE 10\n",
      "Rel. L2 Error = 0.0549, R2 score = 0.997\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 20\n",
      "Rel. L2 Error = 0.0219, R2 score = 0.9995\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 30\n",
      "Rel. L2 Error = 0.0233, R2 score = 0.9995\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 40\n",
      "Rel. L2 Error = 0.0227, R2 score = 0.9995\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 50\n",
      "Rel. L2 Error = 0.0751, R2 score = 0.9944\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 60\n",
      "Rel. L2 Error = 0.0306, R2 score = 0.9991\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 70\n",
      "Rel. L2 Error = 0.0166, R2 score = 0.9997\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 80\n",
      "Rel. L2 Error = 0.023, R2 score = 0.9995\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 90\n",
      "Rel. L2 Error = 0.0285, R2 score = 0.9992\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 100\n",
      "Rel. L2 Error = 0.0313, R2 score = 0.999\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 110\n",
      "Rel. L2 Error = 0.0156, R2 score = 0.9998\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 120\n",
      "Rel. L2 Error = 0.0273, R2 score = 0.9993\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 130\n",
      "Rel. L2 Error = 0.0261, R2 score = 0.9993\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 140\n",
      "Rel. L2 Error = 0.0834, R2 score = 0.993\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 150\n",
      "Rel. L2 Error = 0.026, R2 score = 0.9993\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 160\n",
      "Rel. L2 Error = 0.055, R2 score = 0.997\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 170\n",
      "Rel. L2 Error = 0.0287, R2 score = 0.9992\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 180\n",
      "Rel. L2 Error = 0.0237, R2 score = 0.9994\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 190\n",
      "Rel. L2 Error = 0.0532, R2 score = 0.9972\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 200\n",
      "Rel. L2 Error = 0.0245, R2 score = 0.9994\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 210\n",
      "Rel. L2 Error = 0.0793, R2 score = 0.9937\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 220\n",
      "Rel. L2 Error = 0.0324, R2 score = 0.999\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 230\n",
      "Rel. L2 Error = 0.056, R2 score = 0.9969\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 240\n",
      "Rel. L2 Error = 0.0519, R2 score = 0.9973\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 250\n",
      "Rel. L2 Error = 0.0188, R2 score = 0.9996\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 260\n",
      "Rel. L2 Error = 0.0155, R2 score = 0.9998\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 270\n",
      "Rel. L2 Error = 0.019, R2 score = 0.9996\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 280\n",
      "Rel. L2 Error = 0.0209, R2 score = 0.9996\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 290\n",
      "Rel. L2 Error = 0.0201, R2 score = 0.9996\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 300\n",
      "Rel. L2 Error = 0.0168, R2 score = 0.9997\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 310\n",
      "Rel. L2 Error = 0.0157, R2 score = 0.9998\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 320\n",
      "Rel. L2 Error = 0.0187, R2 score = 0.9997\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 330\n",
      "Rel. L2 Error = 0.0169, R2 score = 0.9997\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 340\n",
      "Rel. L2 Error = 0.0172, R2 score = 0.9997\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 350\n",
      "Rel. L2 Error = 0.0299, R2 score = 0.9991\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 360\n",
      "Rel. L2 Error = 0.0195, R2 score = 0.9996\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 370\n",
      "Rel. L2 Error = 0.0931, R2 score = 0.9913\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 380\n",
      "Rel. L2 Error = 0.0253, R2 score = 0.9994\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 390\n",
      "Rel. L2 Error = 0.0731, R2 score = 0.9947\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 400\n",
      "Rel. L2 Error = 0.025, R2 score = 0.9994\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 410\n",
      "Rel. L2 Error = 0.0274, R2 score = 0.9992\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 420\n",
      "Rel. L2 Error = 0.0189, R2 score = 0.9996\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 430\n",
      "Rel. L2 Error = 0.0311, R2 score = 0.999\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 440\n",
      "Rel. L2 Error = 0.0228, R2 score = 0.9995\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 450\n",
      "Rel. L2 Error = 0.0202, R2 score = 0.9996\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 460\n",
      "Rel. L2 Error = 0.0519, R2 score = 0.9973\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 470\n",
      "Rel. L2 Error = 0.0219, R2 score = 0.9995\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 480\n",
      "Rel. L2 Error = 0.0592, R2 score = 0.9965\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 490\n",
      "Rel. L2 Error = 0.0146, R2 score = 0.9998\n",
      "######################################################################################################################################################################################################################################\n",
      "TEST SAMPLE 500\n",
      "Rel. L2 Error = 0.0275, R2 score = 0.9992\n",
      "######################################################################################################################################################################################################################################\n",
      "Mean Squared Error Test:\n",
      " 6.877497443258562e-05\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "mse_list = []\n",
    "\n",
    "for i in range(inputs_test.shape[0]):\n",
    "    \n",
    "    branch_inputs = inputs_test[i].reshape(1, nx) # (bs, m) = (1, nx) \n",
    "    trunk_inputs = grid # (neval, 2) = (nt*nx, 2)\n",
    "\n",
    "    prediction_i = model(branch_inputs, trunk_inputs).cpu() # (bs, neval) = (1, nt*nx)\n",
    "    target_i = outputs_test[i].reshape(1, -1).cpu()\n",
    "    mse_i = F.mse_loss(prediction_i, target_i)\n",
    "    mse_list.append(mse_i.item())\n",
    "    \n",
    "\n",
    "    if (i+1) % 10 == 0:\n",
    "        print(colored('TEST SAMPLE '+str(i+1), 'red'))\n",
    "        \n",
    "        r2score = metrics.r2_score(outputs_test[i].flatten().cpu().detach().numpy(), prediction_i.flatten().cpu().detach().numpy()) \n",
    "        relerror = np.linalg.norm(outputs_test[i].flatten().cpu().detach().numpy() - prediction_i.flatten().cpu().detach().numpy()) / np.linalg.norm(outputs_test[i].flatten().cpu().detach().numpy())\n",
    "        r2score = float('%.4f'%r2score)\n",
    "        relerror = float('%.4f'%relerror)\n",
    "        print('Rel. L2 Error = '+str(relerror)+', R2 score = '+str(r2score))\n",
    "        \n",
    "        fig = plt.figure(figsize=(15,4))\n",
    "        plt.subplots_adjust(left = 0.1, bottom = 0.1, right = 0.9, top = 0.5, wspace = 0.4, hspace = 0.1)\n",
    "        \n",
    "        ax = fig.add_subplot(1, 4, 1)    \n",
    "        ax.plot(x_span.cpu().detach().numpy(), inputs_test[i].cpu().detach().numpy())\n",
    "        ax.set_xlabel(r'$x$')\n",
    "        ax.set_ylabel(r'$s(t=0, x)$')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        ax = fig.add_subplot(1, 4, 2)  \n",
    "        plt.pcolor(X.cpu().detach().numpy(), T.cpu().detach().numpy(), outputs_test[i].cpu().detach().numpy(), cmap='jet')\n",
    "        plt.colorbar()\n",
    "        ax.set_xlabel(r'$x$')\n",
    "        ax.set_ylabel(r'$t$')\n",
    "        plt.title('$True \\ field$',fontsize=14)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        ax = fig.add_subplot(1, 4, 3)  \n",
    "        plt.pcolor(X.cpu().detach().numpy(), T.cpu().detach().numpy(), prediction_i.reshape(nt, nx).cpu().detach().numpy(), cmap='jet')\n",
    "        plt.colorbar()\n",
    "        ax.set_xlabel(r'$x$')\n",
    "        ax.set_ylabel(r'$t$')\n",
    "        plt.title('$Predicted \\ field$',fontsize=14)  \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        ax = fig.add_subplot(1, 4, 4)  \n",
    "        plt.pcolor(X.cpu().detach().numpy(), T.cpu().detach().numpy(), np.abs(outputs_test[i].cpu().detach().numpy() - prediction_i.reshape(nt, nx).cpu().detach().numpy()), cmap='jet')\n",
    "        plt.colorbar()\n",
    "        ax.set_xlabel(r'$x$')\n",
    "        ax.set_ylabel(r'$t$')\n",
    "        plt.title('$Absolute \\ error$',fontsize=14)  \n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save == True:\n",
    "            plt.savefig(os.path.join(resultdir,'Test_Sample_'+str(i+1)+'.pdf'))\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "        if save == False:\n",
    "            plt.show()\n",
    "\n",
    "        print(colored('#'*230, 'green'))\n",
    "\n",
    "mse = sum(mse_list) / len(mse_list)\n",
    "print(\"Mean Squared Error Test:\\n\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c131947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if modeltype == 'original_kan':\n",
    "    print('##################################################')\n",
    "    print(\"Visualizing the trained KAN branch and trunk nets.\")\n",
    "    branch_net.plot()\n",
    "    trunk_net.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ba6d8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time (sec) to complete:\n",
      "4036.9164423942566\n"
     ]
    }
   ],
   "source": [
    "print(\"Time (sec) to complete:\\n\" +str(finish)) # time for network to train\n",
    "if save == True and cluster == True:\n",
    "    print (\"------END------\")\n",
    "    sys.stdout = orig_stdout\n",
    "    q.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
